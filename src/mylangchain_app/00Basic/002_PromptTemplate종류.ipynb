{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate \n",
    "* [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html#langchain_core.prompts.prompt.PromptTemplate)\n",
    "* [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#langchain_core.prompts.chat.ChatPromptTemplate)\n",
    "* [ChatMessagePromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatMessagePromptTemplate.html#langchain_core.prompts.chat.ChatMessagePromptTemplate)\n",
    "* [FewShotPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html#langchain_core.prompts.few_shot.FewShotPromptTemplate)\n",
    "* PartialPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry add python-dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) PromptTemplate ì˜ from_template() í•¨ìˆ˜ ì‚¬ìš©\n",
    "* ì£¼ë¡œ LLM(í…ìŠ¤íŠ¸ ì™„ì„±í˜• ëª¨ë¸, ex. Ollama, GPT-3.5)ê³¼ í•¨ê»˜ ì‚¬ìš©\n",
    "* í•˜ë‚˜ì˜ ë¬¸ìì—´ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ChatGPTëŠ” ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ **ìê¸°ì§€ë„ í•™ìŠµ**(selfâ€‘supervised learning) ë°©ì‹ìœ¼ë¡œ ì‚¬ì „ '\n",
      " 'í•™ìŠµë©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ëª¨ë¸ì€ ë¬¸ë§¥ì„ ì˜ˆì¸¡í•˜ëŠ” ëª©í‘œë¥¼ ê°€ì§€ê³ , ì£¼ì–´ì§„ ë‹¨ì–´ ì‹œí€€ìŠ¤ì˜ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë§ì¶”ë„ë¡ ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤. '\n",
      " 'ì´í›„ì—ëŠ” íŠ¹ì • ì‘ì—…ì— ë§ê²Œ **ë¯¸ì„¸ì¡°ì •**(fineâ€‘tuning)í•˜ê±°ë‚˜ ì¸ê°„ í”¼ë“œë°±ì„ í™œìš©í•œ ê°•í™”í•™ìŠµ(RLHF)ìœ¼ë¡œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pprint import pprint\n",
    "\n",
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3})\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) PromptTemplate ê²°í•©í•˜ê¸°\n",
    "* ë™ì¼í•œ Prompt íŒ¨í„´ì„ ì‚¬ìš©í•˜ì§€ë§Œ ì—¬ëŸ¬ ê°œì˜ ì§ˆë¬¸ì„ ì‘ì„±í•´ì„œ LLMì„ ì‹¤í–‰í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['count', 'language', 'model_name'] input_types={} partial_variables={} template='{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\\n\\n ê·¸ë¦¬ê³  {model_name} ëª¨ë¸ì˜ ì¥ì ì„ ìš”ì•½ ì •ë¦¬í•´ ì£¼ì„¸ìš”\\n\\n {model_name} ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ AI ëª¨ë¸ì€ ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”? ëª¨ë¸ëª…ì€ {language}ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.'\n",
      "('**ChatGPT ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ (3ë¬¸ì¥)**  \\n'\n",
      " '1. ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ Transformer ê¸°ë°˜ì˜ ì‹ ê²½ë§ì„ ì‚¬ì „ í•™ìŠµ(preâ€‘training)í•˜ì—¬, ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ '\n",
      " 'ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.  \\n'\n",
      " '2. ì‚¬ì „ í•™ìŠµ ë‹¨ê³„ì—ì„œ ì–»ì€ ì¼ë°˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ, íŠ¹ì • ì‘ì—…ì— ë§ê²Œ ì†ŒëŸ‰ì˜ ë¼ë²¨ ë°ì´í„°ë‚˜ ì¸ê°„ í”¼ë“œë°±ì„ í™œìš©í•´ ë¯¸ì„¸ '\n",
      " 'ì¡°ì •(fineâ€‘tuning)í•©ë‹ˆë‹¤.  \\n'\n",
      " '3. ì¸ê°„ì˜ ì„ í˜¸ë„ë¥¼ ë°˜ì˜í•œ ê°•í™”í•™ìŠµ(RLHF) ê³¼ì •ì„ ê±°ì³, ë³´ë‹¤ ì•ˆì „í•˜ê³  ìœ ìš©í•œ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ ìµœì í™”í•©ë‹ˆë‹¤.  \\n'\n",
      " '\\n'\n",
      " '---\\n'\n",
      " '\\n'\n",
      " '### ChatGPT ëª¨ë¸ì˜ ì£¼ìš” ì¥ì   \\n'\n",
      " '- **ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•œ í­ë„“ì€ ì§€ì‹**: ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ë¥¼ í•™ìŠµí•´ ì¼ë°˜ ìƒì‹, ì „ë¬¸ ë¶„ì•¼, ë¬¸í™”Â·ì–¸ì–´ ë“± ë‹¤ì–‘í•œ ì˜ì—­ì„ '\n",
      " 'ì»¤ë²„í•©ë‹ˆë‹¤.  \\n'\n",
      " '- **ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„**: ë¬¸ë§¥ì„ ì¥ê¸°ì ìœ¼ë¡œ ìœ ì§€í•˜ë©´ì„œ ì¼ê´€ëœ í†¤ê³¼ ë…¼ë¦¬ë¡œ ë‹µë³€ì„ ìƒì„±í•´ ì¸ê°„ê³¼ ìœ ì‚¬í•œ ëŒ€í™” ê²½í—˜ì„ '\n",
      " 'ì œê³µí•©ë‹ˆë‹¤.  \\n'\n",
      " '- **ë¹ ë¥¸ ì ì‘ì„±ê³¼ í™•ì¥ì„±**: ë¯¸ì„¸ ì¡°ì •ì´ë‚˜ í”„ë¡¬í”„íŠ¸ ì„¤ê³„ë§Œìœ¼ë¡œë„ ìƒˆë¡œìš´ ì—…ë¬´, ì–¸ì–´, ìŠ¤íƒ€ì¼ ë“±ì— ë¹ ë¥´ê²Œ ì ìš©í•  ìˆ˜ ìˆì–´ ë‹¤ì–‘í•œ '\n",
      " 'ì‚°ì—…Â·ì„œë¹„ìŠ¤ì— í™œìš©í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.  \\n'\n",
      " '\\n'\n",
      " '---\\n'\n",
      " '\\n'\n",
      " '### ChatGPTì™€ ë¹„ìŠ·í•œ AI ëª¨ë¸ (í•œêµ­ì–´ ëª…ì¹­)  \\n'\n",
      " '- **ë¼ë§ˆ (LLaMA)** â€“ ë©”íƒ€ì—ì„œ ê°œë°œí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸  \\n'\n",
      " '- **ì•ŒíŒŒì½”ë“œ (AlphaCode)** â€“ ì½”ë“œ ìƒì„±ì— íŠ¹í™”ëœ DeepMind ëª¨ë¸  \\n'\n",
      " '- **ì½”íˆì–´ëŸ°ìŠ¤ (Cohere)** â€“ ìì—°ì–´ ì²˜ë¦¬ìš© ëŒ€í˜• ì–¸ì–´ ëª¨ë¸  \\n'\n",
      " '- **íŒŒì¸íŠœë‹ëœ GPTâ€‘NeoX** â€“ EleutherAIê°€ ê³µê°œí•œ ì˜¤í”ˆì†ŒìŠ¤ GPT ê³„ì—´ ëª¨ë¸  \\n'\n",
      " '- **ë²„í„°í”Œë¼ì´ (BLOOM)** â€“ ë‹¤êµ­ì–´ ì§€ì›ì„ ëª©í‘œë¡œ í•œ ëŒ€ê·œëª¨ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸  \\n'\n",
      " '- **ì¹´ì´ë¡œìŠ¤ (KAIROS)** â€“ í•œêµ­ì–´ì— íŠ¹í™”ëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(ì˜ˆ: ì¹´ì¹´ì˜¤Â·ë„¤ì´ë²„ ë“±ì—ì„œ ê°œë°œ)  \\n'\n",
      " '\\n'\n",
      " 'ì´ ëª¨ë¸ë“¤ì€ ëª¨ë‘ Transformer êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ê³ , ë‹¤ì–‘í•œ ì–¸ì–´ ìƒì„±Â·ì´í•´ ì‘ì—…ì— í™œìš©ë©ë‹ˆë‹¤.')\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# í…œí”Œë¦¿ì— ê°’ì„ ì±„ì›Œì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì™„ì„±\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# ë¬¸ìì—´ í…œí”Œë¦¿ ê²°í•© (PromptTemplate + PromptTemplate + ë¬¸ìì—´)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n ê·¸ë¦¬ê³  {model_name} ëª¨ë¸ì˜ ì¥ì ì„ ìš”ì•½ ì •ë¦¬í•´ ì£¼ì„¸ìš”\")\n",
    "              + \"\\n\\n {model_name} ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ AI ëª¨ë¸ì€ ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”? ëª¨ë¸ëª…ì€ {language}ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"í•œêµ­ì–´\")\n",
    "print(combined_prompt)\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"í•œêµ­ì–´\"})\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PromptTemplate ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë°°ì—´ í˜•íƒœë¡œ í•˜ì—¬ ì—¬ëŸ¬ê°œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 3 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.', 'Gemma ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 4 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.', 'Gemini ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 4 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.', 'claude ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 3 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.']\n",
      "<class 'str'> GPT-4 ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 3 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
      "('GPT-4ëŠ” ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í™œìš©í•´ í† í° ë‹¨ìœ„ì˜ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµë˜ëŠ” ìê¸° íšŒê·€ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. í•™ìŠµ ê³¼ì •ì—ì„œëŠ” '\n",
      " 'íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì˜ ë‹¤ì¸µ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•´ ë¬¸ë§¥ì„ ì¥ê¸°ì ìœ¼ë¡œ ì´í•´í•˜ê³ , ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ '\n",
      " 'ì¶•ì ëœ íŒ¨í„´ê³¼ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì…ë ¥ì— ë”°ë¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.')\n",
      "<class 'str'> Gemma ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 4 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
      "('Gemma ëª¨ë¸ì€ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•´ **ìê¸°ì§€ë„í•™ìŠµ(selfâ€‘supervised learning)** ë°©ì‹ìœ¼ë¡œ ì‚¬ì „ '\n",
      " 'í•™ìŠµë©ë‹ˆë‹¤.  \\n'\n",
      " 'ì…ë ¥ ë¬¸ì¥ì˜ ì¼ë¶€ í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ê±°ë‚˜ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•˜ì—¬, ëª¨ë¸ì´ ë¬¸ë§¥ì„ ì´í•´í•˜ê³  ì–¸ì–´ íŒ¨í„´ì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.  \\n'\n",
      " 'ì´í›„ **ì§€ë„í•™ìŠµ(supervised fineâ€‘tuning)** ë‹¨ê³„ì—ì„œ íŠ¹ì • ì‘ì—…(ì˜ˆ: ì§ˆë¬¸â€‘ë‹µë³€, ìš”ì•½ ë“±)ì— ë§ëŠ” ë¼ë²¨ì´ ìˆëŠ” '\n",
      " 'ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.  \\n'\n",
      " 'ë§ˆì§€ë§‰ìœ¼ë¡œ **RLHF(ì¸ê°„ í”¼ë“œë°±ì„ í™œìš©í•œ ê°•í™”í•™ìŠµ)**ë¥¼ ì ìš©í•´ ì¸ê°„ í‰ê°€ìì˜ ì„ í˜¸ì— ë§ëŠ” ì¶œë ¥ì„ ìƒì„±í•˜ë„ë¡ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤.')\n",
      "<class 'str'> Gemini ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 4 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
      "('Gemini ëª¨ë¸ì€ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ì™€ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ë¥¼ ì‚¬ì „í•™ìŠµ(Preâ€‘training)í•˜ì—¬ ì–¸ì–´ì™€ ì´ë¯¸ì§€ ë“± ë‹¤ì–‘í•œ ì…ë ¥ í˜•ì‹ì„ '\n",
      " 'ì´í•´í•˜ë„ë¡ í›ˆë ¨ë©ë‹ˆë‹¤.  \\n'\n",
      " 'ê·¸ í›„, ì¸ê°„ í”¼ë“œë°±ì„ í™œìš©í•œ ê°•í™”í•™ìŠµ(RLHF) ë‹¨ê³„ì—ì„œ ëª¨ë¸ì˜ ë‹µë³€ í’ˆì§ˆê³¼ ì•ˆì „ì„±ì„ ê°œì„ í•©ë‹ˆë‹¤.  \\n'\n",
      " 'í•™ìŠµ ê³¼ì •ì—ì„œëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì…€í”„â€‘ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•´ ë¬¸ë§¥ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.  \\n'\n",
      " 'ë§ˆì§€ë§‰ìœ¼ë¡œ, ë„ë©”ì¸â€‘íŠ¹í™” ë°ì´í„°ì™€ ì§€ì†ì ì¸ ì—…ë°ì´íŠ¸ë¥¼ í†µí•´ ìµœì‹  ì§€ì‹ê³¼ íŠ¹ìˆ˜ ì‘ì—… ìˆ˜í–‰ ëŠ¥ë ¥ì„ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.')\n",
      "<class 'str'> claude ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 3 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n",
      "('Claude ëª¨ë¸ì€ ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ ìê¸°ì§€ë„í•™ìŠµ(selfâ€‘supervised learning) ë°©ì‹ìœ¼ë¡œ ì‚¬ì „ '\n",
      " 'í•™ìŠµ(preâ€‘training)ë©ë‹ˆë‹¤.  \\n'\n",
      " 'ê·¸ í›„, ì¸ê°„ í”¼ë“œë°±ì„ í™œìš©í•œ ê°•í™”í•™ìŠµ(RLHF, Reinforcement Learning from Human Feedback) ë‹¨ê³„ì—ì„œ '\n",
      " 'ì§ˆë¬¸â€‘ë‹µë³€, ìš”ì•½ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì— ë§ì¶° ëª¨ë¸ì˜ ì¶œë ¥ì„ ì¡°ì •í•©ë‹ˆë‹¤.  \\n'\n",
      " 'ì´ ë‘ ë‹¨ê³„ê°€ ê²°í•©ë¼ ì–¸ì–´ ì´í•´ì™€ ìƒì„± ëŠ¥ë ¥ì„ ë™ì‹œì— ê°–ì¶˜ ëŒ€í™”í˜• AIê°€ ì™„ì„±ë©ë‹ˆë‹¤.')\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 3},\n",
    "    {\"model_name\": \"Gemma\", \"count\": 4},\n",
    "    {\"model_name\": \"Gemini\", \"count\": 4},\n",
    "    {\"model_name\": \"claude\", \"count\": 3}\n",
    "]\n",
    "\n",
    "# ì—¬ëŸ¬ ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë¯¸ë¦¬ ìƒì„±\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # ë¯¸ë¦¬ ìƒì„±ëœ ì§ˆë¬¸ ëª©ë¡ í™•ì¸\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    print(type(prompt), prompt)\n",
    "    response = llm.invoke(prompt)\n",
    "    pprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) ChatPromptTemplate\n",
    "* Tuple í˜•íƒœì˜ system, user, assistant ë©”ì‹œì§€ ì§€ì›\n",
    "* ì—¬ëŸ¬ ê°œì˜ ë©”ì‹œì§€ë¥¼ ì¡°í•©í•˜ì—¬ LLMì—ê²Œ ì „ë‹¬ ê°€ëŠ¥\n",
    "* ê°„ê²°ì„±ê³¼ ê°€ë…ì„±ì´ ë†’ê³  ë‹¨ìˆœí•œ êµ¬ì¡°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-íŠœí”Œ í˜•íƒœì˜ ë©”ì‹œì§€ ëª©ë¡ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„± (type, content)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    # role, message\n",
    "    (\"system\", \"This system is an expert in answering questions about {topic}. Please provide clear and detailed explanations.\"),\n",
    "    (\"human\", \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"),\n",
    "])\n",
    "\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", model_name=\"ChatGPT\")\n",
    "print(messages)\n",
    "\n",
    "# ìƒì„±í•œ ë©”ì‹œì§€ë¥¼ ë°”ë¡œ ì£¼ì…í•˜ì—¬ í˜¸ì¶œí•˜ê¸°\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(type(response))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²´ì¸ì„ ìƒì„±í•˜ì—¬ í˜¸ì¶œí•˜ê¸°\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"topic\":\"AI\", \"model_name\":\"ChatGPT\"})\n",
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) ChatPromptTemplate\n",
    "* SystemMessagePromptTemplateì™€ HumanMessagePromptTemplate í´ë˜ìŠ¤ ì‚¬ìš©\n",
    "* ê°ì²´ ì§€í–¥ì  ì ‘ê·¼ - Message ê°ì²´ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ìƒì„± ê°€ëŠ¥\n",
    "* ì—¬ëŸ¬ ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„ íƒ\n",
    "\n",
    "```python\n",
    "if user_is_beginner:\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\"ì´ˆë³´ìë¥¼ ìœ„í•œ ì„¤ëª…: {topic}\")\n",
    "else:\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\"ì „ë¬¸ê°€ë¥¼ ìœ„í•œ ìƒì„¸ ë¶„ì„: {topic}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ë”¥ëŸ¬ë‹ì´ë€?  \n",
      "\n",
      "ë”¥ëŸ¬ë‹(Deep Learning)ì€ **ì¸ê³µì‹ ê²½ë§(Artificial Neural Network, ANN)**ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë¨¸ì‹ ëŸ¬ë‹(Machine Learning) ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ, **ë‹¤ì¸µ êµ¬ì¡°(multiâ€‘layer)ì™€ ëŒ€ê·œëª¨ ë°ì´í„°**ë¥¼ í™œìš©í•´ ë³µì¡í•œ íŒ¨í„´ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. â€œë”¥(Deep)â€ì´ë¼ëŠ” ë§ì€ **ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µ(hidden layer)ì´ ë§ì•„ ê¹Šì´ê°€ ê¹Šë‹¤ëŠ” ì˜ë¯¸**ì´ë©°, ì´ ê¹Šì€ êµ¬ì¡°ê°€ ì´ë¯¸ì§€, ìŒì„±, ìì—°ì–´ ë“± ë‹¤ì–‘í•œ ë¹„ì •í˜• ë°ì´í„°ë¥¼ ê³ ì„±ëŠ¥ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ ì¤ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. ê¸°ë³¸ ê°œë…\n",
      "\n",
      "| ìš©ì–´ | ì„¤ëª… |\n",
      "|------|------|\n",
      "| **ì¸ê³µì‹ ê²½ë§ (Neural Network)** | ì¸ê°„ ë‡Œì˜ ë‰´ëŸ°ì„ ëª¨ë°©í•œ ìˆ˜í•™ì  ëª¨ë¸. ì…ë ¥ â†’ ê°€ì¤‘ì¹˜Â·í¸í–¥ â†’ í™œì„±í™” í•¨ìˆ˜ â†’ ì¶œë ¥ ì˜ íë¦„ì„ ê°–ëŠ”ë‹¤. |\n",
      "| **ì¸µ (Layer)** | ì…ë ¥ì¸µ, ì€ë‹‰ì¸µ, ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ë¶„. ì€ë‹‰ì¸µì´ ë§ì„ìˆ˜ë¡ â€œê¹Šì€â€ ë„¤íŠ¸ì›Œí¬ê°€ ëœë‹¤. |\n",
      "| **ê°€ì¤‘ì¹˜ (Weight) / í¸í–¥ (Bias)** | í•™ìŠµ ê³¼ì •ì—ì„œ ì¡°ì •ë˜ëŠ” íŒŒë¼ë¯¸í„°. ì…ë ¥ê°’ì— ê³±í•´ì ¸ ì¶œë ¥ì— ì˜í–¥ì„ ì¤€ë‹¤. |\n",
      "| **í™œì„±í™” í•¨ìˆ˜ (Activation Function)** | ë¹„ì„ í˜•ì„±ì„ ë¶€ì—¬í•´ ë³µì¡í•œ í•¨ìˆ˜ ê·¼ì‚¬ ê°€ëŠ¥í•˜ê²Œ í•¨. ëŒ€í‘œ: ReLU, Sigmoid, Tanh, Softmax ë“±. |\n",
      "| **ì „ë°©ì „íŒŒ (Forward Propagation)** | ì…ë ¥ì´ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µê³¼í•˜ë©´ì„œ ì¶œë ¥ì´ ê³„ì‚°ë˜ëŠ” ê³¼ì •. |\n",
      "| **ì—­ì „íŒŒ (Backpropagation)** | ì¶œë ¥ê³¼ ì •ë‹µ ì‚¬ì´ì˜ ì˜¤ì°¨ë¥¼ ì—­ìœ¼ë¡œ ì „íŒŒí•´ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜. |\n",
      "| **ì†ì‹¤ í•¨ìˆ˜ (Loss Function)** | ëª¨ë¸ ì¶œë ¥ê³¼ ì‹¤ì œ ì •ë‹µ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ìˆ˜ì¹˜í™”. ì˜ˆ: MSE, Crossâ€‘Entropy ë“±. |\n",
      "| **ì˜µí‹°ë§ˆì´ì € (Optimizer)** | ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë„ë¡ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” ë°©ë²•. ì˜ˆ: SGD, Adam, RMSprop ë“±. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. ì™œ â€œê¹Šì€â€ ë„¤íŠ¸ì›Œí¬ê°€ ì¤‘ìš”í•œê°€?\n",
      "\n",
      "1. **í‘œí˜„ í•™ìŠµ (Representation Learning)**  \n",
      "   - ì–•ì€(í•œë‘ ì¸µ) ë„¤íŠ¸ì›Œí¬ëŠ” ì›ì‹œ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ê°„ë‹¨í•œ ë³€í™˜ë§Œ ê°€ëŠ¥í•˜ì§€ë§Œ, ê¹Šì€ ë„¤íŠ¸ì›Œí¬ëŠ” **ê³„ì¸µì ìœ¼ë¡œ íŠ¹ì§•ì„ ì¶”ì¶œ**í•©ë‹ˆë‹¤.  \n",
      "   - ì˜ˆ) ì´ë¯¸ì§€ â†’ ê°€ì¥ ë‚®ì€ ì¸µ: ì—ì§€(edge) ê²€ì¶œ â†’ ì¤‘ê°„ ì¸µ: í…ìŠ¤ì²˜, íŒ¨í„´ â†’ ë†’ì€ ì¸µ: ê°ì²´ í˜•íƒœ.\n",
      "\n",
      "2. **ë¹„ì„ í˜•ì„±ì˜ ì¡°í•©**  \n",
      "   - ì—¬ëŸ¬ ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë¥¼ ê²¹ì³ ì“°ë©´, **ë³µì¡í•œ ë¹„ì„ í˜• í•¨ìˆ˜**ë¥¼ ê·¼ì‚¬í•  ìˆ˜ ìˆì–´ ì „í†µì ì¸ ì„ í˜• ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ê°•ë ¥í•©ë‹ˆë‹¤.\n",
      "\n",
      "3. **íŒŒë¼ë¯¸í„° ê³µìœ ì™€ ì§€ì—­ ì—°ê²°**  \n",
      "   - CNN(Convolutional Neural Network) ê°™ì€ êµ¬ì¡°ëŠ” **í•„í„°(ê°€ì¤‘ì¹˜)ë¥¼ ê³µìœ **í•´ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ í¬ê²Œ ì¤„ì´ë©´ì„œë„ ê³µê°„ì  íŠ¹ì§•ì„ ì˜ í¬ì°©í•©ë‹ˆë‹¤.\n",
      "\n",
      "4. **ëŒ€ê·œëª¨ ë°ì´í„°ì™€ ì—°ì‚° ìì›ì˜ í™œìš©**  \n",
      "   - í˜„ëŒ€ GPUÂ·TPUì™€ ê°™ì€ ë³‘ë ¬ ì—°ì‚° ì¥ì¹˜ë¥¼ ì´ìš©í•´ ìˆ˜ì–µ~ìˆ˜ì‹­ì–µ ê°œ íŒŒë¼ë¯¸í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. ì£¼ìš” ë”¥ëŸ¬ë‹ ëª¨ë¸êµ°\n",
      "\n",
      "| ëª¨ë¸ | íŠ¹ì§• | ì£¼ëœ í™œìš© ë¶„ì•¼ |\n",
      "|------|------|----------------|\n",
      "| **CNN (Convolutional Neural Network)** | 2â€‘D/3â€‘D ì»¨ë³¼ë£¨ì…˜, í’€ë§, ì±„ë„ì„ ì´ìš©í•´ ê³µê°„ì  íŒ¨í„´ì„ í•™ìŠµ | ì´ë¯¸ì§€Â·ì˜ìƒ ì¸ì‹, ê°ì²´ íƒì§€, ì˜ë£Œ ì˜ìƒ |\n",
      "| **RNN (Recurrent Neural Network)** | ìˆœí™˜ ì—°ê²°ì„ í†µí•´ ì‹œê³„ì—´Â·ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ ê¸°ì–µ | ìŒì„± ì¸ì‹, ì–¸ì–´ ëª¨ë¸, ì‹œê³„ì—´ ì˜ˆì¸¡ |\n",
      "| **LSTM / GRU** | ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ í•´ê²° (ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜) | ë²ˆì—­, í…ìŠ¤íŠ¸ ìƒì„±, ì£¼ê°€ ì˜ˆì¸¡ |\n",
      "| **Transformer** | selfâ€‘attention ê¸°ë°˜, ë³‘ë ¬ ì²˜ë¦¬ì— ê°•í•¨ | ìì—°ì–´ ì²˜ë¦¬(NLP), ì´ë¯¸ì§€â€‘í…ìŠ¤íŠ¸ ë©€í‹°ëª¨ë‹¬, ì½”ë“œ ìƒì„± |\n",
      "| **GAN (Generative Adversarial Network)** | ìƒì„±ìì™€ íŒë³„ìì˜ ê²½ìŸì„ í†µí•´ ê³ í’ˆì§ˆ í•©ì„± ë°ì´í„° ìƒì„± | ì´ë¯¸ì§€Â·ë™ì˜ìƒ ìƒì„±, ìŠ¤íƒ€ì¼ ë³€í™˜, ë°ì´í„° ì¦ê°• |\n",
      "| **Autoencoder / Variational Autoencoder (VAE)** | ì…ë ¥ì„ ì••ì¶•Â·ë³µì›í•´ ì ì¬ í‘œí˜„ í•™ìŠµ | ì°¨ì› ì¶•ì†Œ, ì´ìƒ íƒì§€, ì´ë¯¸ì§€ ë³µì› |\n",
      "| **Graph Neural Network (GNN)** | ê·¸ë˜í”„ êµ¬ì¡° ë°ì´í„°ë¥¼ ì§ì ‘ ì²˜ë¦¬ | ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„, í™”í•™ ë¶„ì ì˜ˆì¸¡, ì¶”ì²œ ì‹œìŠ¤í…œ |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. ë”¥ëŸ¬ë‹ í•™ìŠµ íë¦„ (ì¼ë°˜ì ì¸ íŒŒì´í”„ë¼ì¸)\n",
      "\n",
      "1. **ë°ì´í„° ìˆ˜ì§‘Â·ì „ì²˜ë¦¬**  \n",
      "   - ë¼ë²¨ë§, ì •ê·œí™”(0~1, -1~1), ë°ì´í„° ì¦ê°•(ì´ë¯¸ì§€ íšŒì „Â·í¬ë¡­Â·ìƒ‰ìƒ ë³€í˜• ë“±)  \n",
      "2. **ëª¨ë¸ ì„¤ê³„**  \n",
      "   - ì¸µ ì¢…ë¥˜Â·ìˆ˜, í™œì„±í™” í•¨ìˆ˜, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì„ íƒ  \n",
      "3. **í•™ìŠµ (Training)**  \n",
      "   - **ë°°ì¹˜ í•™ìŠµ**(miniâ€‘batch) â†’ ì „ë°©ì „íŒŒ â†’ ì†ì‹¤ ê³„ì‚° â†’ ì—­ì „íŒŒ â†’ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸  \n",
      "   - **ì—í¬í¬(Epoch)**: ì „ì²´ ë°ì´í„°ì…‹ì„ í•œ ë²ˆ ìˆœíšŒ  \n",
      "   - **í•™ìŠµë¥ (Learning Rate) ìŠ¤ì¼€ì¤„ë§**: ì´ˆê¸° í° ê°’ â†’ ì ì§„ì  ê°ì†Œ ë“±  \n",
      "4. **ê²€ì¦Â·íŠœë‹**  \n",
      "   - ê²€ì¦ ë°ì´í„°ë¡œ ê³¼ì í•©(overfitting) ì—¬ë¶€ í™•ì¸, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •  \n",
      "   - ì •ê·œí™” ê¸°ë²•: Dropout, L2 ì •ê·œí™”, ë°°ì¹˜ ì •ê·œí™”(BatchNorm) ë“±  \n",
      "5. **í…ŒìŠ¤íŠ¸Â·ë°°í¬**  \n",
      "   - ìµœì¢… ì„±ëŠ¥ í‰ê°€ í›„, ì¶”ë¡ (inference) ìµœì í™”(ì–‘ìí™”, í”„ë£¨ë‹) í›„ ì„œë¹„ìŠ¤ì— ì ìš©  \n",
      "\n",
      "---\n",
      "\n",
      "## 5. ë”¥ëŸ¬ë‹ì´ ì‹¤ì œë¡œ ì“°ì´ëŠ” ëŒ€í‘œì ì¸ ì‚¬ë¡€\n",
      "\n",
      "| ë¶„ì•¼ | êµ¬ì²´ì  í™œìš© ì˜ˆì‹œ |\n",
      "|------|----------------|\n",
      "| **ì»´í“¨í„° ë¹„ì „** | ì–¼êµ´ ì¸ì‹, ììœ¨ì£¼í–‰ ì°¨ëŸ‰ì˜ ê°ì²´ íƒì§€, ì˜ë£Œ ì˜ìƒì—ì„œ ì•” ì§„ë‹¨ |\n",
      "| **ìì—°ì–´ ì²˜ë¦¬** | ì±—ë´‡Â·ëŒ€í™”í˜• AI(ì˜ˆ: ChatGPT), ê¸°ê³„ ë²ˆì—­, ê°ì„± ë¶„ì„, ë¬¸ì„œ ìš”ì•½ |\n",
      "| **ìŒì„±Â·ìŒì•…** | ìŒì„± ì¸ì‹(ASR), í…ìŠ¤íŠ¸â€‘ìŒì„± ë³€í™˜(TTS), ìŒì•… ìƒì„±Â·ë³´ì • |\n",
      "| **ê²Œì„Â·ì‹œë®¬ë ˆì´ì…˜** | ê°•í™”í•™ìŠµ ê¸°ë°˜ ê²Œì„ AI(AlphaGo, OpenAI Five) |\n",
      "| **ìƒëª…ê³¼í•™** | ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡(AlphaFold), ìœ ì „ì ë°œí˜„ ë°ì´í„° ë¶„ì„ |\n",
      "| **ì‚°ì—…Â·ì œì¡°** | ê²°í•¨ ê²€ì¶œ, ì˜ˆì¸¡ ìœ ì§€ë³´ìˆ˜, ë¡œë´‡ ì œì–´ |\n",
      "| **ê¸ˆìœµ** | ì‹ ìš© ì ìˆ˜ ì˜ˆì¸¡, ì‚¬ê¸° íƒì§€, ì•Œê³ ë¦¬ì¦˜ íŠ¸ë ˆì´ë”© |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. ë”¥ëŸ¬ë‹ì„ ì‹œì‘í•  ë•Œ í”íˆ ë§ˆì£¼ì¹˜ëŠ” ì–´ë ¤ì›€ê³¼ í•´ê²° íŒ\n",
      "\n",
      "| ë¬¸ì œ | ì›ì¸ | í•´ê²° íŒ |\n",
      "|------|------|--------|\n",
      "| **ê³¼ì í•©** | ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ë„ˆë¬´ ë§ì¶°ì§ | - ë” ë§ì€ ë°ì´í„° í™•ë³´<br>- Dropout, L2 ì •ê·œí™” ì ìš©<br>- ì¡°ê¸° ì¢…ë£Œ(Early Stopping) |\n",
      "| **í•™ìŠµì´ ì•ˆ ëœë‹¤** (Lossê°€ ê°ì†Œ ì•ˆ í•¨) | í•™ìŠµë¥ ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìŒ, ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ë¬¸ì œ | - í•™ìŠµë¥ ì„ 10ë°° ì •ë„ ì¡°ì •<br>- He, Xavier ì´ˆê¸°í™” ì‚¬ìš© |\n",
      "| **GPU ë©”ëª¨ë¦¬ ë¶€ì¡±** | ë°°ì¹˜ í¬ê¸°Â·ëª¨ë¸ í¬ê¸° ê³¼ë‹¤ | - ë°°ì¹˜ í¬ê¸° ê°ì†Œ<br>- ëª¨ë¸ ì••ì¶•(í”„ë£¨ë‹, ì–‘ìí™”) |\n",
      "| **ë°ì´í„° ë¶ˆê· í˜•** | íŠ¹ì • í´ë˜ìŠ¤ê°€ í˜„ì €íˆ ì ìŒ | - í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©<br>- ì˜¤ë²„ìƒ˜í”Œë§Â·ì–¸ë”ìƒ˜í”Œë§ |\n",
      "| **ì‹¤ì‹œê°„ ì¶”ë¡  ì§€ì—°** | ëª¨ë¸ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì—°ì‚° ë³µì¡ | - ê²½ëŸ‰ ëª¨ë¸(MobileNet, EfficientNet) ì‚¬ìš©<br>- TensorRT, ONNX ë“± ìµœì í™” ë„êµ¬ í™œìš© |\n",
      "\n",
      "---\n",
      "\n",
      "## 7. ì•ìœ¼ë¡œì˜ íë¦„ê³¼ ì—°êµ¬ íŠ¸ë Œë“œ\n",
      "\n",
      "1. **ëŒ€ê·œëª¨ ì‚¬ì „í•™ìŠµ & íŒŒì¸íŠœë‹**  \n",
      "   - GPTâ€‘4, PaLM, LLaMA ë“± ìˆ˜ì‹­ì–µÂ·ìˆ˜ì¡° íŒŒë¼ë¯¸í„° ëª¨ë¸ì„ ì‚¬ì „í•™ìŠµ í›„, íŠ¹ì • ì‘ì—…ì— ë§ê²Œ ì‘ì€ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ì‹ì´ í‘œì¤€í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **ë©€í‹°ëª¨ë‹¬ í•™ìŠµ**  \n",
      "   - í…ìŠ¤íŠ¸Â·ì´ë¯¸ì§€Â·ìŒì„±Â·ë¹„ë””ì˜¤ë¥¼ ë™ì‹œì— ë‹¤ë£¨ëŠ” ëª¨ë¸(ì˜ˆ: CLIP, Flamingo)ìœ¼ë¡œ, í•˜ë‚˜ì˜ ëª¨ë¸ì´ ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì…ë ¥ì„ ì´í•´í•˜ê³  ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **íš¨ìœ¨ì„±Â·ì¹œí™˜ê²½ AI**  \n",
      "   - íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì´ë©´ì„œë„ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ” **ìŠ¤íŒŒìŠ¤(sparse) ëª¨ë¸**, **ì§€ì‹ ì¦ë¥˜(knowledge distillation)**, **ì–‘ìí™”(quantization)** ë“± ì—°êµ¬ê°€ í™œë°œí•©ë‹ˆë‹¤.\n",
      "\n",
      "4. **ì„¤ëª… ê°€ëŠ¥ì„± & ì•ˆì „ì„±**  \n",
      "   - ì™œ íŠ¹ì • ê²°ì •ì„ ë‚´ë ¸ëŠ”ì§€ ì„¤ëª…í•˜ëŠ” **XAI(eXplainable AI)** ê¸°ë²•ê³¼, í¸í–¥Â·ìœ í•´ ì½˜í…ì¸ ë¥¼ ì–µì œí•˜ëŠ” **AI ì•ˆì „** ì—°êµ¬ê°€ ê°•ì¡°ë©ë‹ˆë‹¤.\n",
      "\n",
      "5. **ìƒì„± ëª¨ë¸ì˜ í™•ì¥**  \n",
      "   - í…ìŠ¤íŠ¸â€‘ì´ë¯¸ì§€Â·í…ìŠ¤íŠ¸â€‘ë¹„ë””ì˜¤Â·í…ìŠ¤íŠ¸â€‘3D ë“± **ë©€í‹°ëª¨ë‹¬ ìƒì„±** ëª¨ë¸ì´ ì‹¤ìš© ë‹¨ê³„ì— ê°€ê¹Œì›Œì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 8. ê°„ë‹¨íˆ ì§ì ‘ ì‹¤ìŠµí•´ ë³´ê¸° (Python + PyTorch ì˜ˆì‹œ)\n",
      "\n",
      "```python\n",
      "# 1ï¸âƒ£ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
      "# pip install torch torchvision\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torchvision\n",
      "import torchvision.transforms as transforms\n",
      "\n",
      "# 2ï¸âƒ£ ë°ì´í„° ë¡œë“œ (MNIST ì†ê¸€ì”¨)\n",
      "transform = transforms.Compose([transforms.ToTensor(),\n",
      "                                transforms.Normalize((0.5,), (0.5,))])\n",
      "train_set = torchvision.datasets.MNIST(root='./data',\n",
      "                                       train=True,\n",
      "                                       download=True,\n",
      "                                       transform=transform)\n",
      "train_loader = torch.utils.data.DataLoader(train_set,\n",
      "                                           batch_size=64,\n",
      "                                           shuffle=True)\n",
      "\n",
      "# 3ï¸âƒ£ ê°„ë‹¨í•œ CNN ëª¨ë¸ ì •ì˜\n",
      "class SimpleCNN(nn.Module):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)   # 28x28 â†’ 28x28\n",
      "        self.pool  = nn.MaxPool2d(2, 2)                          # 28x28 â†’ 14x14\n",
      "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # 14x14 â†’ 14x14\n",
      "        self.fc1   = nn.Linear(64*7*7, 128)\n",
      "        self.fc2   = nn.Linear(128, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.pool(F.relu(self.conv1(x)))   # -> [batch,32,14,14]\n",
      "        x = self.pool(F.relu(self.conv2(x)))   # -> [batch,64,7,7]\n",
      "        x = x.view(-1, 64*7*7)                  # flatten\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = self.fc2(x)\n",
      "        return x\n",
      "\n",
      "model = SimpleCNN()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
      "\n",
      "# 4ï¸âƒ£ í•™ìŠµ ë£¨í”„ (1 epoch)\n",
      "model.train()\n",
      "for images, labels in train_loader:\n",
      "    optimizer.zero_grad()\n",
      "    outputs = model(images)\n",
      "    loss = criterion(outputs, labels)\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "print('í•™ìŠµì´ ëë‚¬ìŠµë‹ˆë‹¤! ë§ˆì§€ë§‰ ë°°ì¹˜ ì†ì‹¤:', loss.item())\n",
      "```\n",
      "\n",
      "> ìœ„ ì½”ë“œëŠ” **MNIST** ë°ì´í„°ì…‹ì— 2ê°œì˜ ì»¨ë³¼ë£¨ì…˜ ì¸µì„ ê°€ì§„ ì‘ì€ CNNì„ í•™ìŠµì‹œí‚¤ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ì˜ˆì‹œì…ë‹ˆë‹¤.  \n",
      "> ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” **ë°ì´í„° ì¦ê°•**, **ê²€ì¦ ë£¨í”„**, **í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§** ë“±ì„ ì¶”ê°€í•´ ì„±ëŠ¥ì„ í¬ê²Œ ëŒì–´ì˜¬ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "## 9. ë§ˆë¬´ë¦¬ ì •ë¦¬\n",
      "\n",
      "- **ë”¥ëŸ¬ë‹ì€** ë‹¤ì¸µ ì‹ ê²½ë§ì„ ì´ìš©í•´ ë³µì¡í•œ ë¹„ì„ í˜• ê´€ê³„ë¥¼ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ê¸°ìˆ ì´ë©°, **ëŒ€ê·œëª¨ ë°ì´í„°ì™€ ê³ ì„±ëŠ¥ ì—°ì‚° ìì›**ì´ ìˆìœ¼ë©´ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.  \n",
      "- í•µì‹¬ì€ **ì¸µ(layer)Â·í™œì„±í™”Â·ê°€ì¤‘ì¹˜Â·ì—­ì „íŒŒ**ë¼ëŠ” ë„¤ ê°€ì§€ ê¸°ë³¸ ìš”ì†Œì´ë©°, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **CNN, RNN, Transformer, GAN** ë“± ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ê°€ íŒŒìƒë˜ì—ˆìŠµë‹ˆë‹¤.  \n",
      "- ì‹¤ì œ ì ìš© ì‹œì—ëŠ” **ë°ì´í„° ì „ì²˜ë¦¬Â·ëª¨ë¸ ì„¤ê³„Â·í•™ìŠµÂ·ê²€ì¦Â·ë°°í¬** ì „ ê³¼ì •ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•´ì•¼ í•˜ë©°, **ê³¼ì í•© ë°©ì§€Â·íš¨ìœ¨ì„± ìµœì í™”** ê°™ì€ ì‹¤ë¬´ì  ì´ìŠˆë¥¼ í•¨ê»˜ ê³ ë¯¼í•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "\n",
      "ê¶ê¸ˆí•œ ì ì´ ë” ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”! ğŸš€\n"
     ]
    }
   ],
   "source": [
    "# ChatMessagePromptTemplate í™œìš©\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê°œë³„ ë©”ì‹œì§€ í…œí”Œë¦¿ ì •ì˜\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI expert in {topic}. Please provide clear and detailed explanations.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplateë¡œ ë©”ì‹œì§€ë“¤ì„ ë¬¶ê¸°\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# ë©”ì‹œì§€ ìƒì„±\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"ë”¥ëŸ¬ë‹ì´ ë­ì•¼?\")\n",
    "\n",
    "# LLM í˜¸ì¶œ\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatMessagePromptTemplateëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ë©”ì‹œì§€(ì‹œìŠ¤í…œ, ì¸ê°„, AI)ë¥¼ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "* SystemMessagePromptTemplate: ì´ í…œí”Œë¦¿ì€ AI ëª¨ë¸ì—ê²Œ ì—­í• ì„ ë¶€ì—¬í•˜ê±°ë‚˜ ì „ë°˜ì ì¸ ê·œì¹™ì„ ì„¤ì •í•˜ëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤. ìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” \"ë²ˆì—­ì„ ë„ì™€ì£¼ëŠ” ìœ ìš©í•œ ë„ìš°ë¯¸\"ë¼ëŠ” ì—­í• ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "* HumanMessagePromptTemplate: ì´ í…œí”Œë¦¿ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì„ ë‹´ëŠ” ì¸ê°„ ë©”ì‹œì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì•„ë˜ì˜ ì˜ˆì‹œì—ì„œëŠ” ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.\n",
    "* ChatPromptTemplate.from_messages: ì´ í´ë˜ìŠ¤ ë©”ì„œë“œëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€, ì¸ê°„ ë©”ì‹œì§€ ë“± ì—¬ëŸ¬ ì¢…ë¥˜ì˜ MessagePromptTemplate ê°ì²´ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ í•˜ë‚˜ì˜ ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.\n",
    "* format_messages: ì´ ë©”ì„œë“œëŠ” ì •ì˜ëœ í…œí”Œë¦¿ì— ì‹¤ì œ ê°’ì„ ì±„ì›Œ ë„£ì–´ [SystemMessage, HumanMessage] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ ë¦¬ìŠ¤íŠ¸ëŠ” ì±„íŒ… ëª¨ë¸(Chat Model) ì— ë°”ë¡œ ì „ë‹¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant that translates English to Korean.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]\n",
      "ë‚˜ëŠ” í”„ë¡œê·¸ë˜ë°ì„ ì¢‹ì•„í•´ìš”.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# 1. SystemMessagePromptTemplateì™€ HumanMessagePromptTemplate ìƒì„±\n",
    "# SystemMessagePromptTemplateëŠ” ëª¨ë¸ì˜ í˜ë¥´ì†Œë‚˜ ë˜ëŠ” ê¸°ë³¸ ì§€ì¹¨ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# HumanMessagePromptTemplateëŠ” ì‚¬ìš©ìë¡œë¶€í„° ë°›ëŠ” ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "human_template = \"{text_to_translate}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# 2. ChatPromptTemplate ìƒì„±\n",
    "# ìœ„ì—ì„œ ë§Œë“  ë‘ í…œí”Œë¦¿ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ ChatPromptTemplateì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# 3. í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…\n",
    "# chat_prompt_template.format_messages()ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# ì´ í•¨ìˆ˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì…ë ¥ ë³€ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤.\n",
    "formatted_prompt = chat_prompt_template.format_messages(\n",
    "    input_language=\"English\",\n",
    "    output_language=\"Korean\",\n",
    "    text_to_translate=\"I love programming.\"\n",
    ")\n",
    "\n",
    "# 4. ê²°ê³¼ ì¶œë ¥\n",
    "print(formatted_prompt)\n",
    "\n",
    "# LLM í˜¸ì¶œ\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) FewShotPromptTemplate\n",
    "* FewShotPromptTemplateì€ ëª¨ë¸ì´ íŠ¹ì • í˜•ì‹ì„ ë”°ë¥´ê²Œ í•˜ê±°ë‚˜, ì¼ê´€ëœ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "* ë„ë©”ì¸ ì§€ì‹ì´ í•„ìš”í•˜ê±°ë‚˜, AIê°€ ì˜¤ë‹µì„ ì¤„ì´ê³  ë” ì‹ ë¢°í•  ë§Œí•œ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ í•´ì•¼ í•  ë•Œ íš¨ê³¼ì ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-1) PromptTemplateì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "íƒœì–‘ê³„ì—ëŠ” 8ê°œì˜ í–‰ì„±ì´ ìˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "1.  ìˆ˜ì„±: íƒœì–‘ê³¼ ê°€ì¥ ê°€ê¹Œìš´ í–‰ì„±ìœ¼ë¡œ, í‘œë©´ì´ ì•”ì„ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆê³  ê·¹ë„ë¡œ ë†’ì€ ì˜¨ë„ì™€ ë‚®ì€ ì˜¨ë„ê°€ ë°˜ë³µë©ë‹ˆë‹¤.\n",
      "2.  ê¸ˆì„±: íƒœì–‘ê³„ì—ì„œ ë‘ ë²ˆì§¸ë¡œ ê°€ê¹Œìš´ í–‰ì„±ìœ¼ë¡œ, ë‘êº¼ìš´ ëŒ€ê¸°ë¡œ ì¸í•´ ê·¹ì‹¬í•œ ì˜¨ì‹¤ íš¨ê³¼ê°€ ë°œìƒí•˜ì—¬ ë§¤ìš° ëœ¨ê²ìŠµë‹ˆë‹¤.\n",
      "3.  ì§€êµ¬: ìš°ë¦¬ê°€ ì‚¬ëŠ” í–‰ì„±ìœ¼ë¡œ, ë¬¼ê³¼ ëŒ€ê¸°ê°€ ìˆì–´ ìƒëª…ì²´ê°€ ì¡´ì¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "4.  í™”ì„±: íƒœì–‘ê³„ì—ì„œ ë„¤ ë²ˆì§¸ë¡œ ê°€ê¹Œìš´ í–‰ì„±ìœ¼ë¡œ, ë¶‰ì€ìƒ‰ì˜ ëª¨ë˜ì‚¬ë§‰ìœ¼ë¡œ ë®ì—¬ ìˆê³  ë¬¼ê³¼ ìƒëª…ì²´ì˜ ì¡´ì¬ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "5.  ëª©ì„±: íƒœì–‘ê³„ì—ì„œ ê°€ì¥ í° í–‰ì„±ìœ¼ë¡œ, ê°€ìŠ¤ ê±°ì¸ì´ë©° ê°•ë ¥í•œ ìê¸°ì¥ê³¼ ìˆ˜ë§ì€ ìœ„ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "6.  í† ì„±: íƒœì–‘ê³„ì—ì„œ ë‘ ë²ˆì§¸ë¡œ í° í–‰ì„±ìœ¼ë¡œ, ê°€ìŠ¤ ê±°ì¸ì´ë©° ì•„ë¦„ë‹¤ìš´ ê³ ë¦¬ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "7.  ì²œì™•ì„±: íƒœì–‘ê³„ì—ì„œ ì¼ê³± ë²ˆì§¸ë¡œ ê°€ê¹Œìš´ í–‰ì„±ìœ¼ë¡œ, ê°€ìŠ¤ ê±°ì¸ì´ë©° ìì „ì¶•ì´ ê¸°ìš¸ì–´ì ¸ ìˆì–´ ê·¹ë‹¨ì ì¸ ê¸°í›„ ë³€í™”ë¥¼ ê²½í—˜í•©ë‹ˆë‹¤.\n",
      "8.  í•´ì™•ì„±: íƒœì–‘ê³„ì—ì„œ ê°€ì¥ ë¨¼ í–‰ì„±ìœ¼ë¡œ, ê°€ìŠ¤ ê±°ì¸ì´ë©° ê°•í•œ ë°”ëŒê³¼ ê·¹ì ì¸ ê¸°í›„ ë³€í™”ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ í–‰ì„±ë“¤ì€ ê°ê° ê³ ìœ í•œ íŠ¹ì§•ê³¼ ì„±ì§ˆì„ ê°€ì§€ê³  ìˆìœ¼ë©°, íƒœì–‘ê³„ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplateì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# chain ì‹¤í–‰\n",
    "result = llm.invoke(\"íƒœì–‘ê³„ì˜ í–‰ì„±ë“¤ì„ ê°„ëµíˆ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\")\n",
    "\n",
    "print(type(result))\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-2) FewShotChatMessagePromptTemplate ì‚¬ìš©í•˜ëŠ” ê²½ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ê³¼í•™ êµìœ¡ìì…ë‹ˆë‹¤.'), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': 'ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.', 'output': '### ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™\\n1. **ê´€ì„±ì˜ ë²•ì¹™**: í˜ì´ ì‘ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë¬¼ì²´ëŠ” ê³„ì† ê°™ì€ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\\n2. **ê°€ì†ë„ì˜ ë²•ì¹™**: ë¬¼ì²´ì— í˜ì´ ì‘ìš©í•˜ë©´, í˜ê³¼ ì§ˆëŸ‰ì— ë”°ë¼ ê°€ì†ë„ê°€ ê²°ì •ë©ë‹ˆë‹¤.\\n3. **ì‘ìš©-ë°˜ì‘ìš© ë²•ì¹™**: ëª¨ë“  í˜ì—ëŠ” í¬ê¸°ê°€ ê°™ê³  ë°©í–¥ì´ ë°˜ëŒ€ì¸ í˜ì´ ì‘ìš©í•©ë‹ˆë‹¤.'}, {'input': 'ì§€êµ¬ì˜ ëŒ€ê¸° êµ¬ì„± ìš”ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.', 'output': '### ì§€êµ¬ ëŒ€ê¸°ì˜ êµ¬ì„±\\n- **ì§ˆì†Œ (78%)**: ëŒ€ê¸°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•©ë‹ˆë‹¤.\\n- **ì‚°ì†Œ (21%)**: ìƒëª…ì²´ê°€ í˜¸í¡í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.\\n- **ì•„ë¥´ê³¤ (0.93%)**: ë°˜ì‘ì„±ì´ ë‚®ì€ ê¸°ì²´ì…ë‹ˆë‹¤.\\n- **ì´ì‚°í™”íƒ„ì†Œ (0.04%)**: ê´‘í•©ì„± ë° ì˜¨ì‹¤ íš¨ê³¼ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]) middle=[] last=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000289268F4950>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000289268C7470>, root_client=<openai.OpenAI object at 0x0000028925CCB200>, root_async_client=<openai.AsyncOpenAI object at 0x00000289268F4500>, model_name='openai/gpt-oss-120b', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')\n",
      "## íƒœì–‘ê³„ í–‰ì„± 8ê°œ (ê°„ë‹¨ ì •ë¦¬)\n",
      "\n",
      "| ìˆœì„œ(íƒœì–‘ì—ì„œ ë©€ì–´ì§€ëŠ” ìˆœì„œ) | í–‰ì„± | í¬ê¸°Â·íŠ¹ì§• | ëŒ€í‘œì ì¸ ì‚¬ì‹¤ |\n",
      "|---|---|---|---|\n",
      "| 1 | **ìˆ˜ì„±** | ê°€ì¥ ì‘ê³  ëœ¨ê±°ìš´ í–‰ì„± | íƒœì–‘ì— ê°€ì¥ ê°€ê¹Œì›Œ í•˜ë£¨ê°€ 58ì¼ ì •ë„ (ìì „ì€ 59ì¼) |\n",
      "| 2 | **ê¸ˆì„±** | â€œì§€êµ¬ì˜ ìŒë‘¥ì´â€ë¼ ë¶ˆë¦¬ì§€ë§Œ ì˜¨ë„ê°€ ë§¤ìš° ë†’ìŒ | ëŒ€ê¸°ê°€ ë‘êº¼ì›Œì„œ ì˜¨ì‹¤ íš¨ê³¼ê°€ ê°•í•´ ì„­ì”¨ 460ë„ê¹Œì§€ ì˜¬ë¼ê° |\n",
      "| 3 | **ì§€êµ¬** | ìƒëª…ì²´ê°€ ì‚¬ëŠ” ìœ ì¼í•œ í–‰ì„± | ë¬¼ì´ ì•¡ì²´ ìƒíƒœë¡œ ì¡´ì¬í•˜ê³ , ëŒ€ê¸°ì—ëŠ” ì‚°ì†Œì™€ ì§ˆì†Œê°€ ë§ìŒ |\n",
      "| 4 | **í™”ì„±** | â€œë¶‰ì€ í–‰ì„±â€ | í‘œë©´ì— í° í™”ì‚°Â·í˜‘ê³¡Â·ê·¹ì§€ ì–¼ìŒì´ ìˆìŒ. ê³¼ê±°ì— ë¬¼ì´ íë¥¸ í”ì ì´ ë°œê²¬ë¨ |\n",
      "| 5 | **ëª©ì„±** | íƒœì–‘ê³„ì—ì„œ ê°€ì¥ í° í–‰ì„± | ê°€ìŠ¤(ìˆ˜ì†ŒÂ·í—¬ë¥¨) í–‰ì„±, ëŒ€ì ì ì´ë¼ëŠ” ê±°ëŒ€í•œ í­í’ì´ 300ë…„ ì´ìƒ ì§€ì† |\n",
      "| 6 | **í† ì„±** | ê³ ë¦¬(ë§)ë¡œ ìœ ëª… | ì•„ë¦„ë‹¤ìš´ ì–¼ìŒê³¼ ì•”ì„ìœ¼ë¡œ ëœ ê³ ë¦¬ê°€ ì—¬ëŸ¬ ê°œ ê²¹ì³ ìˆìŒ |\n",
      "| 7 | **ì²œì™•ì„±** | ì˜†ìœ¼ë¡œ ëˆ„ìš´ í–‰ì„± (ìì „ì¶•ì´ ê±°ì˜ ì˜†ìœ¼ë¡œ ê¸°ìš¸ì–´ì§) | í‘¸ë¥¸ìƒ‰ì€ ë©”íƒ„ ê°€ìŠ¤ ë•Œë¬¸ì—, ë°”ëŒì´ ì•„ì£¼ ë¹ ë¥´ê²Œ ë¶•ëŒ |\n",
      "| 8 | **í•´ì™•ì„±** | ê°€ì¥ ë°”ê¹¥ìª½ ê°€ìŠ¤ í–‰ì„± | ë°”ëŒì´ ê°€ì¥ ë¹ ë¥´ê³ , í‘¸ë¥¸ìƒ‰ì€ ë©”íƒ„ ë•Œë¬¸ì—. íƒœì–‘ê³„ì—ì„œ ê°€ì¥ ì°¨ê°€ìš´ í–‰ì„± ì¤‘ í•˜ë‚˜ |\n",
      "\n",
      "### ê°„ë‹¨ í¬ì¸íŠ¸\n",
      "- **ë‚´í–‰ì„±**(ìˆ˜ì„±Â·ê¸ˆì„±Â·ì§€êµ¬Â·í™”ì„±) â†’ ì•”ì„(ëŒ)ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì‘ì€ í–‰ì„±ë“¤  \n",
      "- **ê°€ìŠ¤ í–‰ì„±**(ëª©ì„±Â·í† ì„±Â·ì²œì™•ì„±Â·í•´ì™•ì„±) â†’ ëŒ€ë¶€ë¶„ì´ ê°€ìŠ¤ë¡œ ì´ë£¨ì–´ì ¸ í¬ê³  ë¬´ê²ë‹¤  \n",
      "- í–‰ì„±ë“¤ì€ **íƒœì–‘ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‹œê³„ ë°©í–¥**(ë¶ë°˜êµ¬ì—ì„œ ë³´ë©´)ìœ¼ë¡œ ëŒê³ , ê°ê° ê³ ìœ í•œ ê³µì „ ì£¼ê¸°ì™€ ìì „ ì£¼ê¸°ê°€ ìˆì–´ìš”.  \n",
      "\n",
      "ì´ ì •ë„ë©´ íƒœì–‘ê³„ í–‰ì„±ë“¤ì„ í•œëˆˆì— ì´í•´í•˜ê¸° ì‰¬ìš¸ ê±°ì˜ˆìš”! ğŸš€ğŸŒ\n"
     ]
    }
   ],
   "source": [
    "# FewShotChatMessagePromptTemplate ì‚¬ìš©í•˜ëŠ” ê²½ìš°\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™\n",
    "1. **ê´€ì„±ì˜ ë²•ì¹™**: í˜ì´ ì‘ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë¬¼ì²´ëŠ” ê³„ì† ê°™ì€ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "2. **ê°€ì†ë„ì˜ ë²•ì¹™**: ë¬¼ì²´ì— í˜ì´ ì‘ìš©í•˜ë©´, í˜ê³¼ ì§ˆëŸ‰ì— ë”°ë¼ ê°€ì†ë„ê°€ ê²°ì •ë©ë‹ˆë‹¤.\n",
    "3. **ì‘ìš©-ë°˜ì‘ìš© ë²•ì¹™**: ëª¨ë“  í˜ì—ëŠ” í¬ê¸°ê°€ ê°™ê³  ë°©í–¥ì´ ë°˜ëŒ€ì¸ í˜ì´ ì‘ìš©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"ì§€êµ¬ì˜ ëŒ€ê¸° êµ¬ì„± ìš”ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ì§€êµ¬ ëŒ€ê¸°ì˜ êµ¬ì„±\n",
    "- **ì§ˆì†Œ (78%)**: ëŒ€ê¸°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•©ë‹ˆë‹¤.\n",
    "- **ì‚°ì†Œ (21%)**: ìƒëª…ì²´ê°€ í˜¸í¡í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.\n",
    "- **ì•„ë¥´ê³¤ (0.93%)**: ë°˜ì‘ì„±ì´ ë‚®ì€ ê¸°ì²´ì…ë‹ˆë‹¤.\n",
    "- **ì´ì‚°í™”íƒ„ì†Œ (0.04%)**: ê´‘í•©ì„± ë° ì˜¨ì‹¤ íš¨ê³¼ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ì˜ˆì œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate ì ìš©\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ê³¼í•™ êµìœ¡ìì…ë‹ˆë‹¤.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ìƒì„± ë° ì²´ì¸ êµ¬ì„±\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "chain = final_prompt | llm\n",
    "print(chain)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "result = chain.invoke({\"input\": \"íƒœì–‘ê³„ì˜ í–‰ì„±ë“¤ì„ ê°„ëµíˆ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-1) PartialPrompt \n",
    "* í”„ë¡¬í”„íŠ¸ë¥¼ ë” ë™ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìœ¼ë©°, AI ì‘ë‹µì„ ë” ì¼ê´€ì„± ìˆê²Œ ì¡°ì • ê°€ëŠ¥í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê³„ì ˆì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ (ë‚¨ë°˜êµ¬/ë¶ë°˜êµ¬ ê³ ë ¤)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "\n",
    "    if hemisphere == \"north\":  # ë¶ë°˜êµ¬ (ê¸°ë³¸ê°’)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ë´„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ì—¬ë¦„\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ê°€ì„\"\n",
    "        else:\n",
    "            return \"ê²¨ìš¸\"\n",
    "    else:  # ë‚¨ë°˜êµ¬ (ê³„ì ˆ ë°˜ëŒ€)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ê°€ì„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ê²¨ìš¸\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ë´„\"\n",
    "        else:\n",
    "            return \"ì—¬ë¦„\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ë¶€ë¶„ ë³€ìˆ˜ ì ìš©)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{season}ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ {phenomenon}ì´ ë§ë‚˜ìš”? {season}ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ì§€êµ¬ê³¼í•™ í˜„ìƒì„ 3ê°œ ì•Œë ¤ì£¼ì„¸ìš”\",\n",
    "    input_variables=[\"phenomenon\"],  # ì‚¬ìš©ì ì…ë ¥ í•„ìš”\n",
    "    partial_variables={\"season\": get_current_season()}  # ë™ì ìœ¼ë¡œ ê³„ì ˆ ê°’ í• ë‹¹\n",
    ")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# íŠ¹ì • ê³„ì ˆì˜ í˜„ìƒ ì§ˆì˜\n",
    "query = prompt.format(phenomenon=\"íƒœí’ ë°œìƒ\")\n",
    "result = llm.invoke(query)\n",
    "\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\" í”„ë¡¬í”„íŠ¸: {query}\")\n",
    "print(f\" ëª¨ë¸ ì‘ë‹µ: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ê³„ì ˆ: ê°€ì„\n",
      "\n",
      " ê°€ì„ì— ë°œìƒí•˜ëŠ” ìì—° í˜„ìƒ:\n",
      "ê°€ì„ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒ 3ê°€ì§€ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1.  **ë¶ë°˜êµ¬ì—ì„œì˜ ê°€ì„ì˜ ë°¤í•˜ëŠ˜** ê°€ì„ì€ ì—¬ë¦„ê³¼ ë¹„êµí–ˆì„ ë•Œ ë°¤ì˜ ì‹œê°„ì´ ê¸¸ì–´ì§€ëŠ” ì‹œê¸°ì´ë©°, ë¶ë°˜êµ¬ì—ì„œ ê°€ì„ ë°¤í•˜ëŠ˜ì„ ê´€ì°°í•  ë•Œ ë¶ë‘ì¹ ì„±ì˜ ëª¨ìŠµì€ ë¶ê·¹ë³„ì¸ í´ë¼ë¦¬ìŠ¤ë¡œ ê°€ëŠ” ê¸¸ì¡ì´ì²˜ëŸ¼ ë³´ì…ë‹ˆë‹¤. \n",
      "2.  **ê°€ì„ì˜ ì€í•˜ìˆ˜** ê°€ì„ ë°¤í•˜ëŠ˜ì—ì„œ ì€í•˜ìˆ˜ë¥¼ ì°¾ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ì„ì˜ ë°¤í•˜ëŠ˜ì—ì„œ ì€í•˜ìˆ˜ë¥¼ ê´€ì¸¡í•  ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” ê°€ì„ ë°¤í•˜ëŠ˜ì˜ ì£¼ìš” ë³„ìë¦¬ë“¤ì´ ë‚¨ìª½ í•˜ëŠ˜ì— ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. \n",
      "3.  **ê°€ì„ì˜ ê¸°ìƒ í˜„ìƒ: ì´ìŠ¬ë¹„** ì´ìŠ¬ë¹„ëŠ” ê°€ì„ì— ìì£¼ ë°œìƒí•˜ëŠ” ê¸°ìƒ í˜„ìƒì…ë‹ˆë‹¤. ì´ìŠ¬ë¹„ëŠ” ê°€ì„ì˜ ì•„ì¹¨ê³¼ ì €ë…ì— ë°œìƒí•˜ëŠ” ì–‡ì€ ì•ˆê°œì™€ ìœ ì‚¬í•œ í˜„ìƒìœ¼ë¡œ, ë°¤ì— ì§€í‘œë©´ì˜ ì˜¨ë„ê°€ ë–¨ì–´ì§€ë©´ì„œ ë¬¼ ë¶„ìê°€ ì‘ê²°ë˜ì–´ ë°œìƒí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë°¤ì— ê¸°ì˜¨ì´ ë‚®ì•„ì§€ë©´ì„œ ì§€í‘œë©´ì—ì„œ ë°œìƒí•œ ìˆ˜ë¶„ì´ ê¸°ì²´ì—ì„œ ì•¡ì²´ë¡œ ë°”ë€Œì–´ ì´ìŠ¬ë°©ìš¸ì´ í˜•ì„±ë˜ë©° ì´ìŠ¬ë¹„ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ í˜„ìƒë“¤ì€ ê°€ì„ì— ìì£¼ ë°œìƒí•˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ê³„ì ˆì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ (ë‚¨ë°˜êµ¬/ë¶ë°˜êµ¬ ê³ ë ¤)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "\n",
    "    if hemisphere == \"north\":  # ë¶ë°˜êµ¬ (ê¸°ë³¸ê°’)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ë´„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ì—¬ë¦„\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ê°€ì„\"\n",
    "        else:\n",
    "            return \"ê²¨ìš¸\"\n",
    "    else:  # ë‚¨ë°˜êµ¬ (ê³„ì ˆ ë°˜ëŒ€)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ê°€ì„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ê²¨ìš¸\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ë´„\"\n",
    "        else:\n",
    "            return \"ì—¬ë¦„\"\n",
    "\n",
    "# Step 1: í˜„ì¬ ê³„ì ˆ ê²°ì •\n",
    "season_name = get_current_season(\"north\")  # ê³„ì ˆ ê°’ ì–»ê¸°\n",
    "print(f\"í˜„ì¬ ê³„ì ˆ: {season_name}\")\n",
    "\n",
    "# Step 2: í•´ë‹¹ ê³„ì ˆì˜ ìì—° í˜„ìƒ ì¶”ì²œ\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{season}ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒ 3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. \"\n",
    "    \"ê° í˜„ìƒì— ëŒ€í•´ ê°„ë‹¨í•œ ì„¤ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì‚¬ìš©\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# ì²´ì¸ 2: ìì—° í˜„ìƒ ì¶”ì²œ (ì…ë ¥: ê³„ì ˆ â†’ ì¶œë ¥: ìì—° í˜„ìƒ ëª©ë¡)\n",
    "chain2 = (\n",
    "    {\"season\": lambda x : season_name}  # chain1ì˜ ì¶œë ¥ì„ season ë³€ìˆ˜ë¡œ ì „ë‹¬\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ì‹¤í–‰: í˜„ì¬ ê³„ì ˆì— ë”°ë¥¸ ìì—° í˜„ìƒ ì¶”ì²œ\n",
    "response = chain2.invoke({})\n",
    "print(f\"\\n {season_name}ì— ë°œìƒí•˜ëŠ” ìì—° í˜„ìƒ:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-2) PartialPrompt \n",
    "* API í˜¸ì¶œ ë°ì´í„°, ì‹œê°„ ì •ë³´, ì‚¬ìš©ì ì •ë³´ ë“±ì„ ë°˜ì˜í•  ë•Œ ë§¤ìš° ìœ ìš©í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " í”„ë¡¬í”„íŠ¸: í˜„ì¬ 1ë‹¬ëŸ¬ = 1377.98ì› ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\n",
      " ëª¨ë¸ ì‘ë‹µ: ìµœê·¼ ì›/ë‹¬ëŸ¬ í™˜ìœ¨ì´ 1,377.98ì› ìˆ˜ì¤€ìœ¼ë¡œ ìƒìŠ¹í•œ ë°°ê²½ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ ê²½ì œ ìš”ì¸ë“¤ì´ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” ì£¼ìš” ë¶„ì„ í¬ì¸íŠ¸ì…ë‹ˆë‹¤.\n",
      "\n",
      "### 1. **ê¸€ë¡œë²Œ ê²½ì œ ìƒí™©**\n",
      "- **ë¯¸êµ­ ê²½ì œì˜ ê²¬ì¡°í•œ ì„±ì¥ì„¸**: ë¯¸êµ­ ê²½ì œê°€ ì—¬ì „íˆ ê²¬ì¡°í•œ ì„±ì¥ì„¸ë¥¼ ë³´ì´ê³  ìˆìœ¼ë©°, ë…¸ë™ì‹œì¥ì´ íƒ„íƒ„í•œ ìƒíƒœë¥¼ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë¯¸êµ­ ì—°ë°©ì¤€ë¹„ì œë„(Fed)ì˜ ê¸´ì¶• ì •ì±…ì„ ë’·ë°›ì¹¨í•˜ê³  ìˆìœ¼ë©°, ê²°ê³¼ì ìœ¼ë¡œ ë‹¬ëŸ¬í™”ì˜ ê°€ì¹˜ë¥¼ ë†’ì´ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "- **ê¸ˆë¦¬ ì¸ìƒ ì§€ì†**: Fedì˜ ê¸ˆë¦¬ ì¸ìƒ ì •ì±…ì´ ë‹¹ë¶„ê°„ ì§€ì†ë  ê²ƒì´ë¼ëŠ” ê¸°ëŒ€ë¡œ ì¸í•´ ë‹¬ëŸ¬í™”ê°€ ë‹¤ë¥¸ í†µí™” ëŒ€ë¹„ ê°•ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ë†’ì€ ê¸ˆë¦¬ëŠ” íˆ¬ììë“¤ì—ê²Œ ë” ë†’ì€ ìˆ˜ìµë¥ ì„ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ë‹¬ëŸ¬í™” ìˆ˜ìš”ë¥¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤.\n",
      "\n",
      "### 2. **êµ­ë‚´ ê²½ì œ ìƒí™©**\n",
      "- **ê²½ê¸° ë‘”í™”**: í•œêµ­ì˜ ê²½ìš°, ê²½ê¸° ë‘”í™” ìš°ë ¤ê°€ ì»¤ì§€ê³  ìˆìŠµë‹ˆë‹¤. ìˆ˜ì¶œ ê°ì†Œ, ë°˜ë„ì²´ ì—…í™© ë¶€ì§„ ë“±ì´ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì›í™” ê°€ì¹˜ í•˜ë½ ìš”ì¸ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
      "- **ì™¸êµ­ì¸ ìê¸ˆ ìœ ì¶œ**: ìµœê·¼ ì™¸êµ­ì¸ íˆ¬ììë“¤ì´ í•œêµ­ ì£¼ì‹ì‹œì¥ì—ì„œ ìê¸ˆì„ ìˆœìœ ì¶œí•˜ê³  ìˆëŠ” ì ë„ ì›í™” ì•½ì„¸ì— ì˜í–¥ì„ ì£¼ê³  ìˆìŠµë‹ˆë‹¤. ì™¸êµ­ì¸ì˜ ìê¸ˆ ìœ ì¶œì€ ì›í™” ìˆ˜ìš” ê°ì†Œë¡œ ì´ì–´ì ¸ í™˜ìœ¨ ìƒìŠ¹ ì••ë ¥ì„ ê°€ì¤‘ì‹œí‚µë‹ˆë‹¤.\n",
      "\n",
      "### 3. **ì—ë„ˆì§€ ê°€ê²© ìƒìŠ¹**\n",
      "- **ìœ ê°€ ìƒìŠ¹**: êµ­ì œ ìœ ê°€ê°€ ìƒìŠ¹ì„¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ í•œêµ­ì˜ ë¬´ì—­ìˆ˜ì§€ì— ë¶€ë‹´ì„ ì£¼ê³  ìˆìŠµë‹ˆë‹¤. ì›ìœ  ìˆ˜ì… ì˜ì¡´ë„ê°€ ë†’ì€ í•œêµ­ì€ ìœ ê°€ ìƒìŠ¹ì´ ê²½ìƒìˆ˜ì§€ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ì›í™” ê°€ì¹˜ì— ë¶€ì •ì ì…ë‹ˆë‹¤.\n",
      "\n",
      "### 4. **í†µí™” ì •ì±… ì°¨ì´**\n",
      "- **í•œêµ­ì€í–‰ì˜ ì™„í™”ì  í†µí™”ì •ì±…**: í•œêµ­ì€í–‰ì´ ì—¬ì „íˆ ì™„í™”ì  í†µí™”ì •ì±…ì„ ìœ ì§€í•˜ê³  ìˆëŠ” ë°˜ë©´, FedëŠ” ê¸´ì¶• ì •ì±…ì„ ì§€ì†í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í†µí™” ì •ì±…ì˜ ì°¨ì´ë„ ë‹¬ëŸ¬í™” ê°•ì„¸ì™€ ì›í™” ì•½ì„¸ë¥¼ ë¶€ì¶”ê¸°ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### ì „ë§\n",
      "- **ë‹¨ê¸°ì ìœ¼ë¡œ ë†’ì€ í™˜ìœ¨ ìœ ì§€**: ì „ë¬¸ê°€ë“¤ì€ ë‹¨ê¸°ì ìœ¼ë¡œ ë†’ì€ í™˜ìœ¨ì´ ìœ ì§€ë  ê°€ëŠ¥ì„±ì´ ìˆë‹¤ê³  ë³´ê³  ìˆìŠµë‹ˆë‹¤. ê¸€ë¡œë²Œ ê²½ì œ ë¶ˆí™•ì‹¤ì„±, ë¯¸êµ­ì˜ ê³ ê¸ˆë¦¬ ì •ì±… ì§€ì†, í•œêµ­ì˜ ê²½ê¸° ë‘”í™” ë“±ì´ ì›í™” ì•½ì„¸ë¥¼ ê³„ì† ê²¬ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- **ì›í™” ë°˜ë“± ê°€ëŠ¥ì„±**: ê·¸ëŸ¬ë‚˜ ì›í™” ë°˜ë“±ì˜ ì—¬ì§€ë„ ìˆìŠµë‹ˆë‹¤. í•œêµ­ ì •ë¶€ì˜ ê²½ì œ ì •ì±… ë°œí‘œ, ìˆ˜ì¶œ ì§€í‘œ ê°œì„ , êµ­ì œ ìœ ê°€ì˜ ì•ˆì •í™” ë“±ì´ ê¸ì •ì ì¸ ì‹ í˜¸ë¡œ ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### íˆ¬ì ë° í™˜ì „ ì „ëµ\n",
      "- **í™˜ìœ¨ ë³€ë™ ìœ„í—˜ ê´€ë¦¬**: ê¸°ì—…ê³¼ ê°œì¸ íˆ¬ììë“¤ì€ í™˜ìœ¨ ë³€ë™ì— ë”°ë¥¸ ìœ„í—˜ì„ ê´€ë¦¬í•˜ê¸° ìœ„í•´ í™˜í—¤ì§€ ìƒí’ˆ ë“±ì„ í™œìš©í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ì˜ˆì¸¡ ê°€ëŠ¥í•œ í™˜ìœ¨ ë³€ë™ì€ ìˆ˜ì¶œì… ê¸°ì—…ë“¤ì—ê²Œ ì¤‘ìš”í•œ ê³ ë ¤ ìš”ì†Œì…ë‹ˆë‹¤.\n",
      "- **ê²½ì œ ì§€í‘œ ëª¨ë‹ˆí„°ë§**: í–¥í›„ ê²½ì œ ì§€í‘œ ë°œí‘œì™€ ê¸€ë¡œë²Œ ê²½ì œ ìƒí™©ì„ ì£¼ì˜ ê¹Šê²Œ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë¯¸êµ­ì˜ CPI, ì‹¤ì—…ë¥  ì§€í‘œì™€ í•œêµ­ì˜ ìˆ˜ì¶œì… ë™í–¥ ë“±ì´ í™˜ìœ¨ì— í° ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ í™˜ìœ¨ ë³€ë™ì€ ë‹¤ì–‘í•œ ê²½ì œ ìš”ì¸ì— ì˜í•´ ì¢Œì§€ìš°ì§€ë˜ë¯€ë¡œ, ìµœì‹  ì •ë³´ë¥¼ ê¾¸ì¤€íˆ í™•ì¸í•˜ê³  ì „ëµì„ ì„¸ìš°ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ì‹¤ì‹œê°„ í™˜ìœ¨ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def get_exchange_rate():\n",
    "    response = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\")\n",
    "    data = response.json()\n",
    "    return f\"1ë‹¬ëŸ¬ = {data['rates']['KRW']}ì›\"\n",
    "\n",
    "# Partial Prompt í™œìš©\n",
    "prompt = PromptTemplate(\n",
    "    template=\"í˜„ì¬ {info} ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\",\n",
    "    input_variables=[],  # ì‚¬ìš©ì ì…ë ¥ ì—†ìŒ\n",
    "    partial_variables={\"info\": get_exchange_rate()}  # APIì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ìë™ ë°˜ì˜\n",
    ")\n",
    "\n",
    "# LLM ëª¨ë¸ ì„¤ì •\n",
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "# ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬ ë° ì‘ë‹µ ë°›ê¸°\n",
    "response = llm.invoke(prompt.format())\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\" í”„ë¡¬í”„íŠ¸:\", prompt.format())\n",
    "print(\" ëª¨ë¸ ì‘ë‹µ:\", response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
