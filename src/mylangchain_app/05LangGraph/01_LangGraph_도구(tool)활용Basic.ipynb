{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangGraph에서 도구(tool) 활용 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 도구(tool)를 활용한 에이전트 개발 방법을 알아봅니다\n",
    "- workflow를 직접 선언하지 않고, 사용가능한 도구들을 전달하면, 에이전트가 적합한 도구를 판단해서 사용합니다\n",
    "    - 이번 회차에서는 `ToolNode`를 통해 도구를 활용하는 방법을 알아봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env 파일을 불러와서 환경 변수로 설정\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "print(UPSTAGE_API_KEY[30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_upstage import ChatUpstage\n",
    "# llm = ChatUpstage(\n",
    "#         model=\"solar-pro\",\n",
    "#         base_url=\"https://api.upstage.ai/v1\",\n",
    "#         temperature=0.5\n",
    "#     )\n",
    "# print(llm)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo-0125', \n",
    "    temperature=0,\n",
    ")\n",
    "print(llm.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"숫자 a와 b를 더합니다.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"숫자 a와 b를 곱합니다.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LangGraph는 `ToolNode`를 통해 도구를 활용합니다\n",
    "- `ToolNode`의 `invoke()`결과는 도구의 `invoke()` 결과와 유사합니다\n",
    "    - 도구는 도구의 실행 결과를 리턴하고, `ToolNode`는 도구의 실행 결과를 포함한 `ToolMessage`를 리턴합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_list = [add, multiply]\n",
    "llm_with_tools = llm.bind_tools(tool_list)\n",
    "\n",
    "tool_node = ToolNode(tool_list)\n",
    "\n",
    "print(type(llm_with_tools))\n",
    "print(tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = multiply.invoke({'a': 3, 'b': 5})\n",
    "\n",
    "print(type(result))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message = llm_with_tools.invoke('What is 3 plus 5?')\n",
    "ai_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ToolNode`를 `invoke()`하려면 `tool_calls` 속성을 포함한 `AIMessage`를 전달해야 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_msg = tool_node.invoke({'messages': [ai_message]})\n",
    "\n",
    "print(type(tool_msg))\n",
    "tool_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 간단한 에이전트를 만들기 위해 LangGraph에서 제공하는 [`StateGraph`](https://langchain-ai.github.io/langgraph/concepts/low_level/#messagesstate)를 사용합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: MessagesState) -> MessagesState:\n",
    "    \"\"\"\n",
    "    에이전트 함수는 주어진 상태에서 메시지를 가져와\n",
    "    LLM과 도구를 사용하여 응답 메시지를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): 메시지 상태를 포함하는 state.\n",
    "\n",
    "    Returns:\n",
    "        MessagesState: 응답 메시지를 포함하는 새로운 state.\n",
    "    \"\"\"\n",
    "    # 상태에서 메시지를 추출합니다.\n",
    "    messages = state['messages']\n",
    "    \n",
    "    # LLM과 도구를 사용하여 메시지를 처리하고 응답을 생성합니다. (RunnableBinding)\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # 응답 메시지를 새로운 상태로 반환합니다.\n",
    "    return {'messages': [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import END\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal['tools', END]:\n",
    "    \"\"\"\n",
    "    주어진 메시지 상태를 기반으로 에이전트가 계속 진행할지 여부를 결정합니다.\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): `state`를 포함하는 객체.\n",
    "\n",
    "    Returns:\n",
    "        Literal['tools', END]: 도구를 사용해야 하면 `tools`를 리턴하고, \n",
    "        답변할 준비가 되었다면 END를 반환해서 프로세스를 종료합니다.\n",
    "    \"\"\"\n",
    "    # 상태에서 메시지를 추출합니다.\n",
    "    messages = state['messages']\n",
    "    print(type(messages))\n",
    "    print(messages)\n",
    "    \n",
    "    # 마지막 AI 메시지를 가져옵니다.\n",
    "    last_ai_message = messages[-1]\n",
    "    \n",
    "    # 마지막 AI 메시지가 도구 호출을 포함하고 있는지 확인합니다.\n",
    "    if last_ai_message.tool_calls:\n",
    "        # 도구 호출이 있으면 'tools'를 반환합니다.\n",
    "        return 'tools'\n",
    "    \n",
    "    # 도구 호출이 없으면 END를 반환하여 프로세스를 종료합니다.\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `node`를 추가하고 `edge`로 연결합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node('agent', agent)\n",
    "#tool_node = ToolNode(tool_list)\n",
    "graph_builder.add_node('tools', tool_node)\n",
    "\n",
    "graph_builder.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END\n",
    "\n",
    "graph_builder.add_edge(START, 'agent')\n",
    "graph_builder.add_conditional_edges(\n",
    "    'agent',\n",
    "    should_continue,\n",
    "    ['tools', END]\n",
    ")\n",
    "graph_builder.add_edge('tools', 'agent')\n",
    "\n",
    "graph_builder.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph= graph_builder.compile()\n",
    "print(type(graph))\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mermaid_code = graph.get_graph().draw_mermaid()\n",
    "print(\"Mermaid Code:\")\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://mermaid.live/ 에서  mermain_code 로 직접 확인한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `graph.stream()`을 활용하면 에이전트가 답변을 생성하는 과정을 모니터링 할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "for chunk in graph.stream({'messages': [HumanMessage('3에다 5를 더하고 거기에 8을 곱하면?')]}, stream_mode='values'):\n",
    "    chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangGraph 내장 ReAct 에이전트 사용\n",
    "* create_react_agent()의 작동 방식\n",
    "    * create_react_agent() 함수 내부에서 이미 StateGraph와 필요한 노드(사고, 도구 실행, 응답 생성 등)를 자동으로 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import tool\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "# llm = ChatUpstage(\n",
    "#         model=\"solar-pro\",\n",
    "#         base_url=\"https://api.upstage.ai/v1\",\n",
    "#         temperature=0.5\n",
    "#     )\n",
    "# print(llm)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo-0125', \n",
    "    temperature=0,\n",
    ")\n",
    "print(llm.model_name)\n",
    "\n",
    "@tool\n",
    "def add2(a: int, b: int) -> int:\n",
    "    \"\"\"숫자 a와 b를 더합니다.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# 한 줄로 에이전트 생성 - 내부에서 StateGraph를 자동 구성\n",
    "\n",
    "agent = create_react_agent(model=llm, tools=[add2])\n",
    "print(type(agent))\n",
    "\n",
    "# 바로 사용 가능\n",
    "result = agent.invoke({\"messages\": [HumanMessage(content=\"3+5는?\")]})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 더 복잡한 도구들 정의\n",
    "@tool\n",
    "def calculate_compound_interest(principal: float, rate: float, years: int) -> str:\n",
    "    \"\"\"원금, 이자율, 기간을 받아 복리 이자를 계산합니다.\"\"\"\n",
    "    amount = principal * (1 + rate/100) ** years\n",
    "    interest = amount - principal\n",
    "    return f\"{years}년 후 원리금: {amount:,.0f}원, 이자: {interest:,.0f}원\"\n",
    "\n",
    "@tool\n",
    "def calculate_tax(income: float) -> str:\n",
    "    \"\"\"연소득을 받아 간이 세액을 계산합니다.\"\"\"\n",
    "    if income <= 12000000:\n",
    "        tax = income * 0.06\n",
    "    elif income <= 46000000:\n",
    "        tax = 720000 + (income - 12000000) * 0.15\n",
    "    elif income <= 88000000:\n",
    "        tax = 5820000 + (income - 46000000) * 0.24\n",
    "    else:\n",
    "        tax = 15900000 + (income - 88000000) * 0.35\n",
    "    \n",
    "    return f\"예상 세액: {tax:,.0f}원, 세후 소득: {income - tax:,.0f}원\"\n",
    "\n",
    "@tool\n",
    "def get_weekday(date_str: str) -> str:\n",
    "    \"\"\"YYYY-MM-DD 형식의 날짜를 받아 요일을 반환합니다.\"\"\"\n",
    "    from datetime import datetime\n",
    "    date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    weekdays = [\"월요일\", \"화요일\", \"수요일\", \"목요일\", \"금요일\", \"토요일\", \"일요일\"]\n",
    "    return f\"{date_str}은 {weekdays[date_obj.weekday()]}입니다.\"\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "print(llm.model_name)\n",
    "\n",
    "# 도구 리스트\n",
    "tools = [calculate_compound_interest, calculate_tax, get_weekday]\n",
    "\n",
    "# ReAct 에이전트 생성\n",
    "agent = create_react_agent(llm, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 복합적인 질문 테스트\n",
    "complex_questions = [\n",
    "    \"1억원을 연 5% 복리로 10년 동안 예금하면 얼마가 될까?\",\n",
    "    \"연소득 5천만원일 때 내야 할 세금은 얼마야?\",\n",
    "    \"2024-12-25은 무슨 요일이야?\",\n",
    "    \"연소득 8천만원일 때 세금을 계산하고, 남은 돈을 연 4% 복리로 5년간 예금하면 얼마가 돼?\"\n",
    "]\n",
    "\n",
    "for question in complex_questions:\n",
    "    print(f\"\\n질문: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result = agent.invoke({\"messages\": [HumanMessage(content=question)]})\n",
    "    response = result[\"messages\"][-1].content\n",
    "    \n",
    "    print(f\"답변: {response}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in complex_questions:\n",
    "    for chunk in agent.stream({'messages': [HumanMessage(content=question)]}, stream_mode='values'):\n",
    "        chunk['messages'][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
