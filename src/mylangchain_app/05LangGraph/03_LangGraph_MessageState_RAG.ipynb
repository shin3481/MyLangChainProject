{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "print(UPSTAGE_API_KEY[30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MessageGraph 역할\n",
    "* MessageGraph는 **채팅 및 대화형 애플리케이션**을 구축하는 데 특화된 그래프 클래스입니다. \n",
    "    * 일반적인 StateGraph가 딕셔너리 기반의 상태를 관리하는 반면, MessageGraph는 대화의 흐름을 나타내는 **메시지 목록(list[messages])**을 핵심 상태로 사용합니다. \n",
    "    * 이를 통해 사용자와 AI 간의 상호작용을 자연스럽게 모델링하고 관리할 수 있습니다.\n",
    "\n",
    "* MessageGraph는 다음과 같은 주요 기능을 제공합니다.\n",
    "    * 메시지 기반 상태 관리: 상태를 messages라는 키를 가진 리스트로 자동 설정하여, 사용자와 AI의 대화 기록을 손쉽게 추적할 수 있습니다.\n",
    "    * 자동 병합: 각 노드에서 새로운 메시지 목록을 반환하면, MessageGraph는 이를 기존 메시지 목록에 자동으로 추가(append)합니다. \n",
    "        * 이는 StateGraph에서 Annotated와 add를 사용해 수동으로 구현해야 했던 기능을 기본적으로 제공합니다.\n",
    "    * 대화 중심 흐름: 대화형 에이전트의 작동 방식을 직관적으로 표현합니다. 한 노드에서 사용자 메시지를 처리하고, 다른 노드에서 AI 응답을 생성하는 등 대화의 각 단계를 명확하게 구분할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Messages State 정의`\n",
    "- 이전 대화 기록을 그래프 상태에 메시지 목록으로 저장하는 것이 유용\n",
    "- 그래프 상태에 Message 객체 목록을 저장하는 키(채널)를 추가하고, 이 키에 리듀서 함수를 추가 \n",
    "- 리듀서 함수 선택:\n",
    "    - operator.add를 사용하면: 새 메시지를 기존 목록에 단순히 추가\n",
    "    - add_messages 함수를 사용하면:\n",
    "        - 새 메시지는 기존 목록에 추가\n",
    "        - 기존 메시지 업데이트도 올바르게 처리 (메시지 ID를 추적)\n",
    "```python\n",
    "class MessageState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]        \n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) RAG Chain 구성`\n",
    "- 메뉴 검색을 위한 벡터저장소를 초기화 (기존 저장소를 로드)\n",
    "- LangChain Runnable로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "# LangGraph MessagesState라는 미리 만들어진 상태를 사용\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from textwrap import dedent\n",
    "from typing import List, Literal, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "\n",
    "# menu db 벡터 저장소 로드\n",
    "menu_db = FAISS.load_local(\n",
    "    \"../db/menu_db\", \n",
    "    embeddings_model, \n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# LLM 모델 \n",
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    ")\n",
    "print(llm.model_name)\n",
    "\n",
    "# RAG 체인 구성\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "system = \"\"\"\n",
    "You are a helpful assistant. Use the following context to answer the user's question:\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 검색기 정의\n",
    "retriever = menu_db.as_retriever(\n",
    "    search_kwargs={\"k\": 6}\n",
    ")\n",
    "\n",
    "query = \"채식주의자를 위한 메뉴를 추천해주세요.\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "# 검색된 문서 출력\n",
    "for doc in retrieved_docs:\n",
    "    print(vars(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RAG 체인 구성\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# RAG 체인 실행\n",
    "query = \"채식주의자를 위한 메뉴를 추천해주세요.\"\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# 답변 출력\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) 노드(Node)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphState(MessagesState):\n",
    "    # messages key는 기본적으로 제공 - 다른 키를 추가하고 싶을 경우 아래 주석과 같이 적용 가능 \n",
    "    documents: List[Document]\n",
    "    grade: float\n",
    "    num_generation: int\n",
    "    \n",
    "# 이 함수는 사용자의 질문을 받아 문서를 검색하고 답변을 생성합니다.\n",
    "def retrieve_and_respond(state: GraphState):\n",
    "    print(\"==>1. retrieve_and_respond\")\n",
    "    # 'messages' 리스트의 가장 마지막 메시지를 가져옵니다.\n",
    "    # state['messages'][-1]은 사용자의 마지막 질문을 가져옵니다.\n",
    "    last_human_message = state['messages'][-1]\n",
    "    \n",
    "    # HumanMessage 객체에서 실제 질문 내용(텍스트)을 가져옵니다.\n",
    "    query = last_human_message.content\n",
    "    \n",
    "    # retriever를 사용하여 쿼리와 관련된 문서를 벡터DB에서 검색합니다.\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    \n",
    "    # RAG 체인(rag_chain)을 사용하여 쿼리에 대한 최종 답변을 생성합니다.\n",
    "    # 이 체인은 검색된 문서를 LLM에 전달하여 답변의 근거로 사용합니다.\n",
    "    response = rag_chain.invoke(query)\n",
    "    \n",
    "    # 검색된 문서와 AI의 응답을 GraphState에 저장하여 반환합니다.\n",
    "    # 'messages' 필드에는 새로운 AI 응답(AIMessage)이 추가됩니다.\n",
    "    # 'documents' 필드에는 검색된 문서 목록이 저장됩니다.\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=response)],\n",
    "        \"documents\": retrieved_docs\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pydantic 모델 정의: \n",
    "* GradeResponse는 LLM의 출력 형식을 **점수(score)**와 **설명(explanation)**을 포함하는 구조로 강제합니다. \n",
    "* 이 모델을 통해 LLM은 정해진 규칙을 따르는 정형화된 JSON 응답을 생성합니다.\n",
    "\n",
    "#### 답변 평가 함수: grade_answer 함수\n",
    "* 정보 추출: state 객체에서 사용자의 **질문(-2)**과 AI의 답변(-1), 그리고 답변의 근거가 된 **문서(documents)**를 가져옵니다.\n",
    "* 프롬프트 생성: LLM에게 평가 전문가 역할을 부여하는 시스템 메시지와 평가에 필요한 모든 정보를 담은 인간 메시지를 포함한 프롬프트를 만듭니다.\n",
    "* 평가 체인 구성: llm.with_structured_output(schema=GradeResponse)를 사용해, LLM이 GradeResponse 모델에 맞춰 응답을 생성하도록 강제합니다.\n",
    "* 평가 실행: 구성된 체인에 질문, 답변, 문서를 입력하여 실행하고, GradeResponse 객체를 얻습니다.\n",
    "* 상태 업데이트: 평가 점수와 현재까지의 답변 생성 횟수를 state에 저장하여 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pydantic을 사용해 LLM 응답의 구조를 정의합니다.\n",
    "# 이 클래스는 LLM이 반환해야 할 데이터 형식을 강제합니다.\n",
    "class GradeResponse(BaseModel):\n",
    "    \"\"\"답변 평가 결과를 나타내는 모델입니다.\"\"\"\n",
    "    \n",
    "    # score 필드는 0.0에서 1.0 사이의 점수를 나타냅니다.\n",
    "    score: float = Field(\n",
    "        #...(Ellipsis, 말줄임표)는 \"이 필드는 필수값이며 기본값이 없음\"을 의미합니다.\n",
    "        ...,\n",
    "        ge=0,  # 0보다 크거나 같음\n",
    "        le=1,  # 1보다 작거나 같음\n",
    "        description=\"0에서 1 사이의 점수, 1은 완벽한 답변을 의미\"\n",
    "    )\n",
    "    \n",
    "    # explanation 필드는 점수에 대한 설명을 담는 문자열입니다.\n",
    "    explanation: str = Field(\n",
    "        ...,\n",
    "        description=\"주어진 점수에 대한 설명\"\n",
    "    )\n",
    "\n",
    "# 답변 품질을 평가하는 함수\n",
    "# 이 함수는 RAG 시스템의 핵심 단계 중 하나로, 생성된 답변을 자체적으로 평가합니다.\n",
    "def grade_answer(state: GraphState):\n",
    "    print(\"==>2. grade_answer\")\n",
    "    # LangGraph의 상태(state)에서 메시지 기록을 가져옵니다.\n",
    "    messages = state['messages']\n",
    "    pprint(messages)\n",
    "    \n",
    "    # 질문과 답변을 추출합니다.\n",
    "    # -2는 사용자의 마지막 질문, -1은 AI의 마지막 답변을 의미합니다.\n",
    "    question = messages[-2].content\n",
    "    print('====>2. question ', type(messages[-2]))\n",
    "    print(question)\n",
    "\n",
    "    answer = messages[-1].content\n",
    "    print('====>2. answer ', type(messages[-1]))\n",
    "    print(answer)\n",
    "    \n",
    "    # 검색된 문서 목록을 가져와 프롬프트에 사용하기 위해 포맷팅합니다.\n",
    "    context = format_docs(state['documents'])\n",
    "    print('====>2. context ')\n",
    "    print(context)\n",
    "\n",
    "    # LLM에게 평가 전문가 역할을 부여하는 시스템 프롬프트입니다.\n",
    "    grading_system = \"\"\"당신은 전문 평가자입니다. 주어진 맥락을 고려하여 질문에 대한 답변의 관련성과 정확성을 평가하세요.\n",
    "    1이 완벽한 점수인 0에서 1 사이의 점수를 설명과 함께 제공하세요.\"\"\"\n",
    "\n",
    "    # LLM이 평가에 사용할 입력 프롬프트 템플릿을 정의합니다.\n",
    "    grading_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", grading_system),\n",
    "        (\"human\", \"[Question]\\n{question}\\n\\n[Context]\\n{context}\\n\\n[Answer]\\n{answer}\\n\\n[Grade]\\n\")\n",
    "    ])\n",
    "    \n",
    "    # 프롬프트와 LLM을 연결하는 체인을 만듭니다.\n",
    "    # .with_structured_output() 메서드는 LLM이 GradeResponse Pydantic 모델에 맞춰 응답하도록 강제합니다.\n",
    "    grading_chain = grading_prompt | llm.with_structured_output(schema=GradeResponse)\n",
    "    \n",
    "    # 평가 체인을 실행하고, LLM의 정형화된 응답을 받습니다.\n",
    "    grade_response = grading_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "\n",
    "    # 답변 생성 시도 횟수를 추적합니다.\n",
    "    num_generation = state.get('num_generation', 0)\n",
    "    num_generation += 1\n",
    "    \n",
    "    print('====>2. grade_response.score ')\n",
    "    print(grade_response.score)\n",
    "    # 평가 점수와 갱신된 생성 횟수를 상태에 저장하여 반환합니다.\n",
    "    return {\"grade\": grade_response.score, \"num_generation\": num_generation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) 엣지(Edge)`\n",
    "* routing 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 이 함수는 RAG 에이전트가 다음 행동을 결정하는 '라우터' 역할을 합니다.\n",
    "# 반환 값은 \"retrieve_and_respond\" 또는 \"generate\"로 고정됩니다.\n",
    "# END 노드에 대한 별칭(alias)으로 \"generate\"로 사용합니다.\n",
    "def should_retry(state: GraphState) -> Literal[\"retrieve_and_respond\", \"generate\"]:\n",
    "    print(\"==>3. should_retry 라우팅함수\")\n",
    "    print(\"----GRADTING---\")\n",
    "    print(\"Grade Score 점수 : \", state[\"grade\"])\n",
    "    print(\"시도횟수 = \", state[\"num_generation\"])\n",
    "\n",
    "    # 답변 생성 시도 횟수를 확인합니다.\n",
    "    # 만약 3번 이상 시도했다면, 더 이상 재시도하지 않고 최종 답변을 생성하도록 합니다.\n",
    "    if state[\"num_generation\"] > 2:\n",
    "        return \"generate\"\n",
    "    \n",
    "    # 답변의 품질 점수를 확인합니다.\n",
    "    # 점수가 0.7 미만이면, 현재 답변이 충분하지 않다고 판단하고\n",
    "    # 문서를 다시 검색하여 답변을 재시도하도록 합니다.\n",
    "    if state[\"grade\"] < 0.7:\n",
    "        return \"retrieve_and_respond\"\n",
    "    else:\n",
    "        # 점수가 0.7 이상이면, 답변이 충분히 좋다고 판단하고\n",
    "        # 최종 답변을 생성하도록 합니다.\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(5) 그래프(Graph) 구성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StateGraph는 그래프의 상태를 관리하는 기본 클래스입니다.\n",
    "# GraphState는 그래프가 공유하는 데이터의 구조를 정의한 사용자정의 클래스입니다.\n",
    "builder = StateGraph(GraphState)\n",
    "\n",
    "# --- Node 정의 ---\n",
    "# 그래프에 두 개의 노드(처리 단계)를 추가합니다.\n",
    "# \"retrieve_and_respond\": 문서를 검색하고 답변을 생성하는 노드입니다.\n",
    "# \"grade_answer\": 생성된 답변의 품질을 평가하는 노드입니다.\n",
    "builder.add_node(\"retrieve_and_respond\", retrieve_and_respond)\n",
    "builder.add_node(\"grade_answer\", grade_answer)\n",
    "\n",
    "# --- Edge(연결) 추가 ---\n",
    "# 그래프의 시작과 끝, 그리고 노드 간의 흐름을 정의합니다.\n",
    "# START에서 시작하여 \"retrieve_and_respond\" 노드로 이동합니다.\n",
    "builder.add_edge(START, \"retrieve_and_respond\")\n",
    "\n",
    "# \"retrieve_and_respond\" 노드에서 \"grade_answer\" 노드로 이동합니다.\n",
    "builder.add_edge(\"retrieve_and_respond\", \"grade_answer\")\n",
    "\n",
    "# --- 조건부 Edge 추가 ---\n",
    "# \"grade_answer\" 노드의 결과에 따라 다음 노드를 동적으로 결정합니다.\n",
    "builder.add_conditional_edges(\n",
    "    # 현재 노드: \"grade_answer\"\n",
    "    \"grade_answer\",\n",
    "    # 라우팅 함수: 'should_retry' 함수가 다음 노드를 결정합니다.\n",
    "    should_retry,\n",
    "    # 매핑: 'should_retry' 함수의 반환 값에 따라 이동할 노드를 정의합니다.\n",
    "    {\n",
    "        # 'should_retry'가 \"retrieve_and_respond\"를 반환하면, 같은 노드로 돌아가 재시도합니다.\n",
    "        \"retrieve_and_respond\": \"retrieve_and_respond\",\n",
    "        # 'should_retry'가 \"generate\"를 반환하면, 그래프 실행을 종료합니다.\n",
    "        # \"generate\"가 'END' 노드의 별칭 역할을 합니다.\n",
    "        \"generate\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- 그래프 컴파일 ---\n",
    "# 정의된 노드와 엣지를 기반으로 실행 가능한 그래프를 만듭니다.\n",
    "# 이 단계는 그래프를 최적화하고 실행 준비를 완료합니다.\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 시각화\n",
    "#display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "mermaid_code = graph.get_graph().draw_mermaid()\n",
    "print(\"Mermaid Code:\")\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://mermaid.live/ 에서  mermain_code 로 직접 확인한다.\n",
    "\n",
    "* [Graph이미지](https://mermaidchart.com/play?utm_source=mermaid_live_editor&utm_medium=share#pako:eNp9kd9ugjAUxl-lOUsWTYBBUcBqvJmPsKuNhVQ5BRIopJT9M777CirRhYyb9mu_8_1OD0c41CkCg0zxJicvu3UsY50krebKLLO3TbMd1eap2b7PGWOiUK3ujQq1KvADEy7TRGHb1DKdTR3Oe7dhpP1p-4lqdivmZyga_xU57Edgyc-8sRVi21syBVr_BRHbIY9y3zbrDCUqrvGsiGMSLpypov_ypy6Gjm4zBufBdN7uUJAUBe9KTURRluxBUOEKYZWFRDvHIss18xx6VzDMeLDbdcMPhf5m7p2hn8klbi_2gTjEkpgPLPMvixSY4GWLFlSoKt5rOPaGGHSOFcbAzPbSVAyxPJm6hsvXuq6AadWZSlV3WX4VXZOa0e0Kbp5YjeHKjA_Vc91JDczzhwhgR_gCFpoHhb5Ll_4q8PzFamnBt_EsIicKaLSiy8hfhDQ6WfAzMF0nDH0aUuoFPnVdP4xOv2Qa5lY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(6) Graph 실행`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 상태\n",
    "#HumanMessage(content=\"스테이크의 요리는 어떤 것들이 있나요?\")\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"채식주의자를 위한 메뉴를 추천해주세요.\")],\n",
    "}\n",
    "\n",
    "# 그래프 실행 \n",
    "final_state = graph.invoke(initial_state)\n",
    "\n",
    "# 최종 상태 출력\n",
    "print(\"최종 상태:\\n\")\n",
    "pprint(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 답변만 출력\n",
    "pprint(final_state['messages'][-1].content) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradio 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import gradio as gr\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# 예시 질문 리스트\n",
    "# Gradio 인터페이스에 미리 보여줄 질문들입니다. 사용자는 이 질문들을 클릭해 바로 테스트할 수 있습니다.\n",
    "example_questions = [\n",
    "    \"채식주의자를 위한 메뉴를 추천해주세요.\",\n",
    "    \"오늘의 스페셜 메뉴는 무엇인가요?\",\n",
    "    \"스테이크 메뉴가 있나요?\"\n",
    "]\n",
    "\n",
    "# 대답 함수 정의\n",
    "# 이 함수는 Gradio의 ChatInterface에 연결되어 사용자의 질문을 처리하고 AI의 응답을 반환합니다.\n",
    "def answer_invoke(message: str, history: List[Tuple[str, str]]) -> str:\n",
    "    try:\n",
    "        # 채팅 기록을 AI 모델이 이해할 수 있는 LangChain 메시지 객체 형식으로 변환합니다.\n",
    "        chat_history = []\n",
    "        for human, ai in history:\n",
    "            chat_history.append(HumanMessage(content=human))\n",
    "            chat_history.append(AIMessage(content=ai))\n",
    "\n",
    "        # LangGraph에 전달할 초기 상태를 구성합니다.\n",
    "        # 최근 2개의 대화 기록과 현재 사용자의 질문을 포함시킵니다.\n",
    "        # 이는 AI가 이전 대화의 맥락을 이해하도록 돕습니다.\n",
    "        initial_state = {\n",
    "            \"messages\": chat_history[-2:] + [HumanMessage(content=message)],\n",
    "        }\n",
    "\n",
    "        # LangGraph를 호출하여 메시지 체인을 실행하고 최종 상태를 얻습니다.\n",
    "        # 이 과정에서 RAG 로직이 수행됩니다.\n",
    "        final_state = graph.invoke(initial_state)\n",
    "        \n",
    "        # 최종 상태에서 가장 마지막에 생성된 메시지(AI의 응답)의 내용을 반환합니다.\n",
    "        return final_state[\"messages\"][-1].content\n",
    "        \n",
    "    except Exception as e:\n",
    "        # 오류 발생 시 사용자에게 친절한 메시지를 반환하고,\n",
    "        # 개발자가 디버깅할 수 있도록 콘솔에 오류를 출력합니다.\n",
    "        print(f\"오류가 발생했습니다: {str(e)}\")\n",
    "        return \"죄송합니다. 응답을 생성하는 동안 오류가 발생했습니다. 다시 시도해 주세요.\"\n",
    "\n",
    "\n",
    "# Gradio 인터페이스 생성\n",
    "# 사용자와 상호작용할 UI를 만듭니다.\n",
    "demo = gr.ChatInterface(\n",
    "    fn=answer_invoke,  # 사용자 입력이 들어왔을 때 실행할 함수\n",
    "    title=\"레스토랑 메뉴 AI 어시스턴트\",  # UI의 제목\n",
    "    description=\"메뉴 정보, 추천, 음식 관련 질문에 답변해 드립니다.\",  # UI의 설명\n",
    "    examples=example_questions,  # 사용자가 쉽게 시작할 수 있도록 제공되는 예시 질문들\n",
    "    theme=gr.themes.Soft()  # 부드러운 색상의 UI 테마 적용\n",
    ")\n",
    "\n",
    "# Gradio 애플리케이션을 실행합니다.\n",
    "# 이 함수를 호출하면 웹 서버가 시작되어 로컬에서 채팅 인터페이스에 접속할 수 있습니다.\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데모 종료\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
