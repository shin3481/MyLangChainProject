{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "print(UPSTAGE_API_KEY[30:])\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "print(TAVILY_API_KEY[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "# LangGraph MessagesState라는 미리 만들어진 상태를 사용\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from textwrap import dedent\n",
    "from typing import List, Literal, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import uuid\n",
    "\n",
    "#from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tool 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 사용자 정의 - @tool decorator`\n",
    "- 메뉴 검색을 위한 벡터저장소를 초기화 (기존 저장소를 로드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "\n",
    "# menu db 벡터 저장소 로드\n",
    "menu_db = FAISS.load_local(\n",
    "    \"../db/menu_db\", \n",
    "    embeddings_model, \n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Tool 정의 \n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def search_menu(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Securely retrieve and access authorized restaurant menu information from the encrypted database.\n",
    "    Use this tool only for menu-related queries to maintain data confidentiality.\n",
    "    레스토랑 메뉴에서 정보를 검색합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = menu_db.similarity_search(query, k=6)\n",
    "\n",
    "    formatted_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if len(docs) > 0:\n",
    "        return formatted_docs\n",
    "    \n",
    "    return \"관련 메뉴 정보를 찾을 수 없습니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) LangChain 내장 도구`\n",
    "- 일반 웹 검색을 위한 Tavily 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tool 정의 \n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def search_web(query: str) -> List[str]:\n",
    "    \"\"\"Searches the internet for information that does not exist in the database or for the latest information.\"\"\"\n",
    "    \n",
    "    tavily_search = TavilySearchResults(max_results=3)\n",
    "    docs = tavily_search.invoke(query)\n",
    "\n",
    "    formatted_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if len(docs) > 0:\n",
    "        return formatted_docs\n",
    "    \n",
    "    return \"관련 정보를 찾을 수 없습니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. bind_tools() 함수로 LLM과 Tool 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM 모델 \n",
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\", streaming=True)\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    ")\n",
    "print(llm.model_name)\n",
    "\n",
    "# 도구 목록\n",
    "tools = [search_menu, search_web]\n",
    "\n",
    "# 모델에 도구를 바인딩 RunnableBindings\n",
    "llm_with_tools = llm.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 도구 호출 ( Vector DB )\n",
    "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"스테이크 메뉴의 가격은 얼마인가요?\")])\n",
    "\n",
    "# 결과 출력\n",
    "pprint(tool_call.additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구 호출 ( Tavily )\n",
    "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"최근에 공개된 오픈소스 LLM 모델은 어떤 것들이 있나요?\")])\n",
    "\n",
    "# 결과 출력\n",
    "pprint(tool_call.additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구 호출 \n",
    "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"3+3은 얼마인가요?\")])\n",
    "\n",
    "# 결과 출력\n",
    "pprint(tool_call.additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(tool_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 도구 노드(ToolNode) \n",
    "- AI 모델이 요청한 도구(tool) 호출을 실행하는 역할을 처리하는 LangGraph 콤포넌트\n",
    "- 작동 방식:\n",
    "    - 가장 최근의 AIMessage에서 도구 호출 요청을 추출 (반드시, AIMessage는 반드시 tool_calls가 채워져 있어야 함)\n",
    "    - 요청된 도구들을 병렬로 실행\n",
    "    - 각 도구 호출에 대해 ToolMessage를 생성하여 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 도구 노드(Tool Node) 정의`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 도구 노드 정의 \n",
    "tools = [search_menu, search_web]\n",
    "tool_node = ToolNode(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구 호출 \n",
    "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"스테이크 메뉴의 가격은 얼마인가요?\")])\n",
    "\n",
    "pprint(tool_call.additional_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 도구 노드(Tool Node) 실행`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구 호출 결과를 메시지로 추가하여 실행 \n",
    "# tool_call 변수는 RunnableBinding 객체 (LLM + tool)\n",
    "results = tool_node.invoke({\"messages\": [tool_call]})\n",
    "\n",
    "# 실행 결과 출력하여 확인 \n",
    "for result in results['messages']:\n",
    "    print(type(result))\n",
    "    print(result.content)\n",
    "    print('**** --------------------------- ****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 모델을 이용하여 도구를 호출하여 실행 \n",
    "results = tool_node.invoke({\"messages\": [llm_with_tools.invoke(\"최근에 공개된 오픈소스 LLM 모델은 어떤 것들이 있나요?\")]})\n",
    "\n",
    "# 실행 결과 출력하여 확인 \n",
    "for result in results['messages']:\n",
    "    print(result.content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ReAct Agent\n",
    "- ReAct(Reasoning and Acting) : 가장 일반적인 에이전트\n",
    "- 동작 방식:\n",
    "    - 행동 (act): 모델이 특정 도구를 호출\n",
    "    - 관찰 (observe): 도구의 출력을 모델에 다시 전달\n",
    "    - 추론 (reason): 모델이 도구 출력을 바탕으로 다음 행동을 결정 (예: 또 다른 도구를 호출하거나 직접 응답을 생성)\n",
    "\n",
    "- 논문: https://arxiv.org/abs/2210.03629"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) LangGraph 내장 ReAct 에이전트 사용`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1) create_react_agent() 함수 사용\n",
    "* create_react_agent() 함수를 사용해서 생성된 agent 를 호출할때 HumanMessage(질문)만 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "#from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "tools = [search_menu, search_web]\n",
    "graph = create_react_agent(\n",
    "    llm, \n",
    "    tools=tools, \n",
    ")\n",
    "\n",
    "print(type(graph))\n",
    "\n",
    "graph\n",
    "# 그래프 출력\n",
    "#display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행\n",
    "inputs = {\"messages\": [HumanMessage(content=\"스테이크 메뉴의 가격은 얼마인가요?\")]}\n",
    "messages = graph.invoke(inputs)\n",
    "\n",
    "print(type(messages))\n",
    "\n",
    "for m in messages['messages']:\n",
    "    #print(type(m))\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"최근에 공개된 오픈소스 LLM 모델은 어떤 것들이 있나요?\")]}\n",
    "messages = graph.invoke(inputs)\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2) create_react_agent() 함수 사용\n",
    "* create_react_agent() 함수를 사용해서 생성된 agent 를 호출할때 HumanMessage(질문)와 SystemMessage(역할부여) 2개의 메시지를 전달함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시스템 프롬프트\n",
    "system_prompt = dedent(\"\"\"\n",
    "You are an AI assistant designed to answer human questions. \n",
    "You can use the provided tools to help generate your responses.\n",
    "\n",
    "Follow these steps to answer questions:\n",
    "    1. Carefully read and understand the question.\n",
    "    2. Use the provided tools to obtain necessary information.\n",
    "    3. Immediately after using a tool, cite the source using the format below.\n",
    "    4. Construct an accurate and helpful answer using the tool outputs and citations.\n",
    "    5. Provide the final answer when you determine it's complete.\n",
    "\n",
    "When using tools, follow this format:\n",
    "    Action: tool_name\n",
    "    Action Input: input for the tool\n",
    "\n",
    "Immediately after receiving tool output, cite the source as follows:\n",
    "    [Source: tool_name | document_title/item_name | url/file_path]\n",
    "\n",
    "For example:\n",
    "    Action: search_menu\n",
    "    Action Input: 스테이크\n",
    "    \n",
    "    (After receiving tool output)\n",
    "    [Source: search_menu | 스테이크 | ./data/data.txt]\n",
    "    스테이크에 대한 정보는 다음과 같습니다...\n",
    "\n",
    "    Action: search_web\n",
    "    Action Input: History of AI\n",
    "\n",
    "    (After receiving tool output)\n",
    "    [Source: search_web | AI History | https://en.wikipedia.org/wiki/History_of_artificial_intelligence]\n",
    "    AI의 역사는 다음과 같이 요약됩니다...\n",
    "\n",
    "If tool use is not necessary, answer directly.\n",
    "\n",
    "Your final answer should be clear, concise, and directly related to the user's question. \n",
    "Ensure that every piece of factual information in your response is accompanied by a citation.\n",
    "\n",
    "Remember: ALWAYS include these citations for all factual information, tool outputs, and referenced documents in your response. \n",
    "Do not provide any information without a corresponding citation.\n",
    "\"\"\")\n",
    "\n",
    "# 그래프 생성 \n",
    "graph = create_react_agent(\n",
    "    llm, \n",
    "    tools=tools, \n",
    ")\n",
    "\n",
    "# 그래프 출력\n",
    "#display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행\n",
    "query = \"스테이크 메뉴의 가격은 얼마인가요?\"\n",
    "#query = \"최근에 공개된 오픈소스 LLM 모델의 성능을 비교해 주세요?\"\n",
    "#query = \"안녕하세요?\"\n",
    "messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=query)\n",
    "]\n",
    "\n",
    "# 현재 graph 변수는 내장형 에이전트를 사용하는 Graph\n",
    "messages = graph.invoke({\"messages\": messages})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 직접 StateGraph객체를 사용해서 tool를 사용하는 Agent 생성하기 ( 내장형 agent를 사용하지 않음 )`\n",
    "- 조건부 엣지 함수를 사용자 정의\n",
    "- `should_continue` 함수에서 도구 호출 여부에 따라 종료 여부를 결정\n",
    "- 도구 실행이 필요한 경우에는 그래프가 종료되지 않고 계속 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GraphState는 LangGraph의 상태를 정의하는 사용자정의 클래스입니다.\n",
    "# LangGraph의 MessagesState를 상속받아 메시지 목록을 자동으로 관리합니다.\n",
    "class GraphState(MessagesState):\n",
    "    pass\n",
    "\n",
    "# --- 노드 구성 ---\n",
    "# call_model 노드는 LLM을 호출하여 응답을 생성합니다.\n",
    "def call_model(state: GraphState):\n",
    "    # 시스템 메시지를 정의하여 LLM의 페르소나와 역할을 설정합니다.\n",
    "    system_message = SystemMessage(content=system_prompt)\n",
    "    # 기존 메시지 목록 앞에 시스템 메시지를 추가합니다.\n",
    "    messages = [system_message] + state['messages']\n",
    "    # 도구 호출 기능이 활성화된 LLM을 호출하여 응답을 받습니다.\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    # LLM의 응답을 상태에 저장하여 반환합니다.\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# should_continue 노드는 LLM 응답을 분석하여 다음 단계를 결정하는 라우터 역할을 합니다.\n",
    "# router 노드 역할을 하는 함수\n",
    "def should_continue(state: GraphState):\n",
    "    # 가장 마지막 메시지를 가져옵니다.\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # 마지막 메시지에 도구 호출이 포함되어 있으면, \"execute_tools\" 노드로 이동합니다.\n",
    "    if last_message.tool_calls:\n",
    "        return \"execute_tools\"\n",
    "    # 도구 호출이 없으면, 대화를 종료합니다.\n",
    "    return END\n",
    "\n",
    "# --- 그래프 구성 ---\n",
    "# 상태를 관리하는 그래프를 생성합니다.\n",
    "builder = StateGraph(GraphState)\n",
    "\n",
    "# 노드들을 그래프에 추가합니다.\n",
    "# \"call_model\": LLM을 호출하여 응답을 받는 노드\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "\n",
    "\"\"\"\n",
    "ToolNode는 도구 실행에 필요한 복잡한 로직을 미리 구현해 놓은 래퍼(wrapper) 클래스입니다. \n",
    "개발자는 단순히 사용하려는 도구 목록(tools)만 전달하면 됩니다.  \n",
    "직접 함수를 정의해야 한다면, 각 도구의 이름과 입력 인자를 파싱하고, 해당하는 도구를 찾아 실행하는 코드를 수동으로 작성해야 합니다. \n",
    "ToolNode는 이 과정을 자동화하여 코드의 양을 크게 줄여줍니다.\n",
    "Tool을 호출하는 함수를 개발자가 직접 정의하지 않고, Tool을 호출하는 함수를 LangGraph Node로 만들어 주는 역할\n",
    "\"\"\"\n",
    "tools = [search_menu, search_web]\n",
    "builder.add_node(\"execute_tools\", ToolNode(tools))\n",
    "\n",
    "builder.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 엣지(연결) 추가 ---\n",
    "# START에서 \"call_model\" 노드로 시작합니다.\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "# \"call_model\" 노드에서 'should_continue' 함수를 사용하여 다음 단계를 결정합니다.\n",
    "builder.add_conditional_edges(\n",
    "    \"call_model\",\n",
    "    should_continue,\n",
    "    {\n",
    "        # 'should_continue'가 \"execute_tools\"를 반환하면, 해당 노드로 이동합니다.\n",
    "        \"execute_tools\": \"execute_tools\",\n",
    "        # 'should_continue'가 END를 반환하면, 그래프를 종료합니다.\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# \"execute_tools\" 노드에서 도구 실행 후, ToolMessage결과를 가지고 다시 \"call_model\" 노드로 돌아갑니다.\n",
    "# 이는 LLM이 도구의 결과를 보고 최종 응답을 생성하도록 합니다.\n",
    "builder.add_edge(\"execute_tools\", \"call_model\")\n",
    "\n",
    "# 그래프를 컴파일하여 실행 가능한 상태로 만듭니다.\n",
    "my_graph = builder.compile()\n",
    "print(type(my_graph))\n",
    "\n",
    "my_graph\n",
    "# 그래프 출력 \n",
    "#display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool를 사용하지 않는 경우 1-> 2-> 3 \n",
    "* 1. SystemMessage\n",
    "* 2. HumanMessage\n",
    "* 3. AIMessage (tool_calls 정보 포함, tool_calls가 비어 있으면 종료)\n",
    "\n",
    "#### Tool를 사용하는 경우 1-> 2-> 3 -> 4 -> 5\n",
    "* 1. SystemMessage\n",
    "* 2. HumanMessage\n",
    "* 3. AIMessage (tool_calls 정보 포함, tool_calls 정보를 기반으로 Tool을 호출)\n",
    "* 4. ToolMessage\n",
    "* 5. AIMessage (최종 결과)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mermaid_code = my_graph.get_graph().draw_mermaid()\n",
    "print(\"Mermaid Code:\")\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://mermaid.live/ 에서  mermain_code 로 직접 확인한다.\n",
    "\n",
    "* [Graph이미지](https://www.mermaidchart.com/play?utm_source=mermaid_live_editor&utm_medium=share#pako:eNptkMsOgjAQRX-lqRtIrA8WLophxSe4U9OUMiNNRiBQEo3x3-UVTKOrzuROz5n2xU2VA5f81ui6YKc0vpQXp1TrdNMfwflYJ0t33NbJNZRSom1aNwwaTaTuPYGCbxkOCTzAdA6UqypqA68LJwWU-SIY6wVPeqIvYiZEwr6C2FczsenjGfI38_Txz3p_8f0SbQrIckDdkWNoieQKI9whrsmWIAqwt8LJ_SbyLoyfM46LqtbGuqfceQPD82ZchtkBDX9_ALpujZU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행\n",
    "#query = \"스테이크 메뉴의 가격은 얼마인가요?\"\n",
    "query = \"최근에 공개된 오픈소스 LLM 모델의 성능을 비교해 주세요?\"\n",
    "#query = \"안녕하세요?\"\n",
    "messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=query)\n",
    "]\n",
    "inputs = {\"messages\": messages}\n",
    "\n",
    "# my_graph 변수는 직접 StateGraph를 생성하고 노드와 엣지를 추가한 graph\n",
    "messages = my_graph.invoke(inputs)\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"최근에 가장 많이 사용되는 오픈소스 Javascript 라이브러리들은 어떤 것들이 있나요?\"\n",
    "messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=query)\n",
    "]\n",
    "inputs = {\"messages\": messages}\n",
    "\n",
    "messages = my_graph.invoke(inputs)\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) 직접 StateGraph객체를 사용해서 tool를 사용하는 Agent 생성하기 ( 내장형 agent를 사용하지 않음, tools_condition() 함수 )\n",
    "- LangGraph에서 제공하는 도구 사용을 위한 조건부 엣지 함수  tools_condition 함수 활용\n",
    "- 최신 메시지(결과)가 도구 호출이면 -> `tools_condition`이 도구로 라우팅\n",
    "- 최신 메시지(결과)가 도구 호출이 아니면 -> `tools_condition`이 `END`로 라우팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# --- 노드 함수 정의 ---\n",
    "# 'call_model' 노드는 LLM을 호출하여 응답을 생성합니다.\n",
    "def call_model(state: GraphState):\n",
    "    # 시스템 프롬프트를 정의하여 LLM의 역할을 설정합니다.\n",
    "    system_message = SystemMessage(content=system_prompt)\n",
    "    # 기존 메시지 기록 앞에 시스템 메시지를 추가합니다.\n",
    "    messages = [system_message] + state['messages']\n",
    "    # 도구 사용이 가능한 LLM을 호출합니다.\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    # LLM의 응답을 상태에 추가합니다.\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# --- 그래프 구성 ---\n",
    "# 상태를 관리하는 그래프 빌더를 생성합니다.\n",
    "builder = StateGraph(GraphState)\n",
    "\n",
    "# 노드들을 그래프에 추가합니다.\n",
    "# \"agent\": LLM을 호출하는 노드입니다.\n",
    "builder.add_node(\"agent\", call_model)\n",
    "# \"tools\": 도구 호출을 실행하는 내장 노드입니다.\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# 그래프의 시작점을 설정합니다.\n",
    "builder.add_edge(START, \"agent\")\n",
    "\n",
    "# 'tools_condition'을 사용한 조건부 엣지 추가\n",
    "# 이 함수는 LLM 응답에 도구 호출이 포함되어 있는지 자동으로 확인하고,\n",
    "# 다음 노드를 'tools' 또는 'END'로 결정합니다.\n",
    "# 별도의 라우팅 함수를 만들 필요가 없어 코드가 간결해집니다.\n",
    "builder.add_conditional_edges(\n",
    "    # 현재 노드: \"agent\" (LLM 응답이 생성된 곳)\n",
    "    \"agent\",\n",
    "    # 라우팅 함수: LangGraph의 'tools_condition'\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "# 도구 실행이 끝난 후, 다시 'agent' 노드로 돌아가서\n",
    "# LLM이 도구의 결과를 보고 최종 답변을 생성하도록 합니다.\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# 그래프를 컴파일하여 실행 가능한 상태로 만듭니다.\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 출력\n",
    "#display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "mermaid_code = graph.get_graph().draw_mermaid()\n",
    "print(\"Mermaid Code:\")\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://mermaid.live/ 에서  mermain_code 로 직접 확인한다.\n",
    "\n",
    "* [Graph이미지](https://mermaidchart.com/play?utm_source=mermaid_live_editor&utm_medium=share#pako:eNpdkc2OgjAQx1-lmb1oAgitAlbjZX2EPe2yMRVaaAItKSVZ1_juW6pLopfpzGT-v_noFUpdcaBQG9Y36OO4K1RhT6fBMuOexde-P8zRftUfvpeUUiHNYKdCVnNlF94up9hq3Q4Lb5d3EFfVjPH-DGnZnTHjURgekGftZjYKI5d8SF_Tvs9u7vuiL12D4cgFqrhgY2uRkG1L3wQWsRBBKxUPGy7rxtIkwk8Cv54vD3XPSmkvNH4qmEZ_4M7inIoSAndAWQEVrB14AB03HZtiuBYKoQJswzteAHXuY5wCCnVzup6pT607oNaMTmn0WDf_wdhXzPKjZO53uhlu3DG4edejskBx4hFAr_ADNHOrZCTGG7JNE7LebgK4AE3WeZSnON_iTU7WGc5vAfz6nnGUZQRnGCcpwXFMsvz2B6g0r_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행\n",
    "inputs = {\"messages\": [HumanMessage(content=\"해산물 메뉴에는 어떤 것들이 있나요?\")]}\n",
    "messages = graph.invoke(inputs)\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MemorySaver\n",
    "\n",
    "1. 상태의 일시성 문제:\n",
    "   - 기본적으로 그래프 실행 시 상태는 일시적 (stateless)\n",
    "   - 그래프를 재실행하는 경우 상태가 초기화되는 문제가 있음 \n",
    "   - 따라서, 중단이 있는 다중 턴 대화가 어려움 \n",
    "\n",
    "2. MemorySaver 기능:\n",
    "   - 가장 쉽게 사용할 수 있는 체크포인터 (각 단계 후 그래프 상태를 자동으로 저장)\n",
    "   - 그래프 상태를 위한 인메모리 키-값 저장소\n",
    "   - 지속성(persistence) 있는 메모리 기능을 제공하여 그래프 객체가 체크포인터부터 이어서 실행 가능 \n",
    "\n",
    "3. 메모리의 필요성:\n",
    "   - 대화의 연속성: 여러 턴에 걸친 대화를 유지 \n",
    "   - 중단 허용: 대화 중 중단이 있어도 이전 상태를 복원\n",
    "   - 유연한 상태 관리: 다양한 대화 스레드를 독립적으로 관리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. 사용자 정의 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행 - 이전 대화 내용을 기억하는지 못하는 문제가 있음 \n",
    "inputs = {\"messages\": [HumanMessage(content=\"이 중에 하나만 추천해주세요.\")]}\n",
    "messages = graph.invoke(inputs)\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 체크포인터 지정`\n",
    "- 그래프를 컴파일할 때 체크포인터를 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 메모리 초기화 \n",
    "memory = MemorySaver()\n",
    "\n",
    "# 체크포인터 지정하여 그래프 컴파일 \n",
    "graph_memory = builder.compile(checkpointer=memory)\n",
    "\n",
    "print(type(builder))\n",
    "print(type(memory))\n",
    "print(type(graph_memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 체크포인터 사용`\n",
    "- 메모리 사용 시 `thread_id`를 지정 \n",
    "- 체크포인터는 그래프의 각 단계에서 상태를 기록 (그래프 각 단계의 모든 상태를 컬렉션으로 저장)\n",
    "- 나중에 `thread_id`를 사용하여 이 스레드에 접근 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages = [HumanMessage(content=\"스테이크 메뉴의 가격은 얼마인가요?\")]\n",
    "messages = graph_memory.invoke({\"messages\": messages}, config)\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages = [HumanMessage(content=\"둘 중에 더 저렴한 메뉴는 무엇인가요?\")]\n",
    "messages = graph_memory.invoke({\"messages\": messages}, config)\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. 내장 ReAct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# 메모리 초기화 \n",
    "memory = MemorySaver()\n",
    "\n",
    "# 그래프 생성 \n",
    "graph = create_react_agent(\n",
    "    llm, \n",
    "    tools=tools, \n",
    "    #state_modifier=system_prompt,\n",
    "    checkpointer=memory,\n",
    ")\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 그래프 출력\n",
    "#display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=\"채식주의자를 위한 메뉴가 있나요?\")\n",
    "    ]\n",
    "\n",
    "messages = graph.invoke({\"messages\": messages}, config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "messages = [HumanMessage(content=\"방금 답변에 버섯이 포함된 메뉴가 있나요?\")]\n",
    "messages = graph.invoke({\"messages\": messages}, config)\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradio 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import List, Tuple\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import uuid\n",
    "\n",
    "# MemorySaver를 사용하여 그래프의 상태를 메모리에 저장합니다.\n",
    "# 이를 통해 이전 대화의 맥락을 기억할 수 있습니다.\n",
    "memory = MemorySaver()\n",
    "\n",
    "# builder를 컴파일할 때 'checkpointer'를 설정하여 메모리 기능을 활성화합니다.\n",
    "graph_memory = builder.compile(checkpointer=memory)\n",
    "\n",
    "# 예시 질문들입니다. Gradio UI에 미리 표시되어 사용자가 쉽게 챗봇을 테스트할 수 있습니다.\n",
    "example_questions = [\n",
    "    \"채식주의자를 위한 메뉴를 추천해주세요.\",\n",
    "    \"오늘의 스페셜 메뉴는 무엇인가요?\",\n",
    "    \"파스타에 어울리는 음료는 무엇인가요?\"\n",
    "]\n",
    "\n",
    "# 사용자의 메시지를 처리하고 응답을 생성하는 핵심 함수입니다.\n",
    "def process_message(message: str, history: List[Tuple[str, str]], thread_id: str) -> str:\n",
    "    try:\n",
    "        # LangGraph의 상태 저장소에 접근하기 위한 설정입니다.\n",
    "        # \"thread_id\"는 각 대화 세션을 고유하게 식별하는 데 사용됩니다.\n",
    "        config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "        \n",
    "        # 그래프에 전달할 초기 입력입니다. 사용자의 현재 메시지를 담고 있습니다.\n",
    "        inputs = {\"messages\": [HumanMessage(content=message)]}\n",
    "        \n",
    "        # 설정된 그래프를 호출하여 전체 워크플로우(예: RAG)를 실행합니다.\n",
    "        # 'graph_memory'는 이전 대화 상태를 자동으로 불러와서 사용합니다.\n",
    "        result = graph_memory.invoke(inputs, config=config)\n",
    "        \n",
    "        # 결과에 'messages'가 포함되어 있으면 응답을 처리합니다.\n",
    "        if \"messages\" in result:\n",
    "            # 현재 스레드 ID와 메시지들을 출력하여 디버깅에 도움을 줍니다.\n",
    "            print(f\"스레드 ID: {thread_id}\")\n",
    "            for msg in result[\"messages\"]:\n",
    "                # 메시지를 깔끔한 형식으로 출력합니다.\n",
    "                msg.pretty_print()\n",
    "\n",
    "            # 마지막 메시지를 가져옵니다.\n",
    "            last_message = result[\"messages\"][-1]\n",
    "            \n",
    "            # 마지막 메시지가 AI의 응답이면 해당 내용을 반환합니다.\n",
    "            if isinstance(last_message, AIMessage):\n",
    "                return last_message.content\n",
    "\n",
    "        # 응답이 유효하지 않을 경우 반환할 기본 메시지입니다.\n",
    "        return \"응답을 생성하지 못했습니다.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # 오류 발생 시 오류 메시지를 출력하고, 사용자에게 알립니다.\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return \"죄송합니다. 응답을 생성하는 동안 오류가 발생했습니다. 다시 시도해 주세요.\"\n",
    "\n",
    "# ChatBot 클래스는 Gradio에 필요한 인터페이스를 제공하고, 각 세션의 thread_id를 관리합니다.\n",
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        # 챗봇 인스턴스마다 고유한 스레드 ID를 생성합니다.\n",
    "        self.thread_id = str(uuid.uuid4())\n",
    "\n",
    "    def chat(self, message: str, history: List[Tuple[str, str]]) -> str:\n",
    "        # 현재 스레드 ID를 출력하여 확인합니다.\n",
    "        print(f\"Thread ID: {self.thread_id}\")\n",
    "        # 'process_message' 함수를 호출하여 실제 메시지 처리를 위임합니다.\n",
    "        response = process_message(message, history, self.thread_id)\n",
    "        return response\n",
    "\n",
    "# ChatBot 클래스의 인스턴스를 생성합니다.\n",
    "chatbot = ChatBot()\n",
    "\n",
    "# Gradio 채팅 인터페이스를 설정하고, 챗봇의 'chat' 메서드를 연결합니다.\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chatbot.chat,  # 사용자가 메시지를 입력하면 호출될 함수\n",
    "    title=\"레스토랑 메뉴 AI 어시스턴트\",\n",
    "    description=\"메뉴 정보, 추천, 음식 관련 질문에 답변해 드립니다. 정보의 출처를 함께 제공합니다.\",\n",
    "    examples=example_questions,\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "# Gradio 애플리케이션을 실행합니다.\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데모 종료\n",
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
