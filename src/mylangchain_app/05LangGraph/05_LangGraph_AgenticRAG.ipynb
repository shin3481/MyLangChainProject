{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b21d4441",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7515c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:2])\n",
    "\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
    "print(UPSTAGE_API_KEY[30:])\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "print(TAVILY_API_KEY[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4394ecdf",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe5c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "#from langchain_core.tools import tool\n",
    "from langchain.agents import tool\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "# LangGraph MessagesState라는 미리 만들어진 상태를 사용\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from textwrap import dedent\n",
    "from typing import List, Literal, Tuple, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import uuid\n",
    "\n",
    "#from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a901480",
   "metadata": {},
   "source": [
    "###  2-1. Tool 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8f6d8",
   "metadata": {},
   "source": [
    "- 메뉴 검색을 위한 벡터저장소를 초기화 (기존 저장소를 로드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "\n",
    "# menu db 벡터 저장소 로드\n",
    "menu_db = FAISS.load_local(\n",
    "    \"../db/menu_db\", \n",
    "    embeddings_model, \n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# wine db 벡터 저장소 로드\n",
    "wine_db = FAISS.load_local(\n",
    "    \"../db/wine_db\", \n",
    "    embeddings_model, \n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "@tool\n",
    "def search_menu(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Securely retrieve and access authorized restaurant menu information from the encrypted database.\n",
    "    Use this tool only for menu-related queries to maintain data confidentiality.\n",
    "    \"\"\"\n",
    "    docs = menu_db.similarity_search(query, k=6)\n",
    "    if len(docs) > 0:\n",
    "        return docs\n",
    "    \n",
    "    return [Document(page_content=\"관련 메뉴 정보를 찾을 수 없습니다.\")]\n",
    "\n",
    "@tool\n",
    "def search_wine(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Securely retrieve and access authorized restaurant wine information from the encrypted database.\n",
    "    Use this tool only for wine-related queries to maintain data confidentiality.\n",
    "    \"\"\"\n",
    "    docs = wine_db.similarity_search(query, k=6)\n",
    "    if len(docs) > 0:\n",
    "        return docs\n",
    "    \n",
    "    return [Document(page_content=\"관련 와인 정보를 찾을 수 없습니다.\")]\n",
    "\n",
    "# 웹 검색 \n",
    "@tool\n",
    "def search_web(query: str) -> List[str]:\n",
    "    \"\"\"Searches the internet for information that does not exist in the database or for the latest information.\"\"\"\n",
    "\n",
    "    tavily_search = TavilySearchResults(max_results=2)\n",
    "    docs = tavily_search.invoke(query)\n",
    "\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        formatted_docs.append(\n",
    "            Document(\n",
    "                page_content= f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>',\n",
    "                metadata={\"source\": \"web search\", \"url\": doc[\"url\"]}\n",
    "                )\n",
    "        )\n",
    "\n",
    "    if len(formatted_docs) > 0:\n",
    "        return formatted_docs\n",
    "    \n",
    "    return [Document(page_content=\"관련 정보를 찾을 수 없습니다.\")]\n",
    "\n",
    "\n",
    "# 도구 목록을 정의 \n",
    "tools = [search_menu, search_wine, search_web]\n",
    "print(type(tools[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7f7a4",
   "metadata": {},
   "source": [
    "### 2-2. LLM 모델\n",
    "* bind_tools() 함수로 model 과 tool 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 기본 LLM\n",
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, streaming=True)\n",
    "\n",
    "from langchain_upstage import ChatUpstage\n",
    "llm = ChatUpstage(\n",
    "        model=\"solar-pro\",\n",
    "        base_url=\"https://api.upstage.ai/v1\",\n",
    "        temperature=0.5\n",
    "    )\n",
    "print(llm.model_name)\n",
    "\n",
    "# LLM에 도구 바인딩하여 추가 \n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "print(type(llm_with_tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fabf018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메뉴 검색에 관련된 질문을 하는 경우 -> 메뉴 검색 도구를 호출  \n",
    "query = \"대표 메뉴는 무엇인가요?\"\n",
    "ai_msg = llm_with_tools.invoke(query)\n",
    "\n",
    "pprint(ai_msg)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "pprint(ai_msg.content)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "pprint(ai_msg.tool_calls)\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689eddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구들의 목적과 관련 없는 질문을 하는 경우 -> 도구 호출 없이 그대로 답변을 생성 \n",
    "query = \"안녕하세요?\"\n",
    "ai_msg = llm_with_tools.invoke(query)\n",
    "\n",
    "pprint(ai_msg)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "pprint(ai_msg.content)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "pprint(ai_msg.tool_calls)\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d80b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹 검색 목적과 관련된 질문을 하는 경우 -> 웹 검색 도구 호출 \n",
    "query = \"2024년 상반기 엔비디아 시가총액은 어떻게 변동 되었나요?\"\n",
    "ai_msg = llm_with_tools.invoke(query)\n",
    "\n",
    "pprint(ai_msg)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "pprint(ai_msg.content)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "pprint(ai_msg.tool_calls)\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa910b6",
   "metadata": {},
   "source": [
    "## 3. Adaptive RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb9f9c",
   "metadata": {},
   "source": [
    "### 3-1. 그래프 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53dacc5",
   "metadata": {},
   "source": [
    "`(1) 상태 정의`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be73335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 Schema 정의 \n",
    "class AdaptiveRagState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    generation: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd6f08",
   "metadata": {},
   "source": [
    "`(2) 질문 분석 -> 라우팅`\n",
    "- 사용자의 질문을 분석하여 적절한 검색 방법을 선택 \n",
    "- 레스토랑 메뉴 검색 or 레스토랑 와인 검색  or 일반 웹 검색 or 단순 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9830629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from textwrap import dedent\n",
    "\n",
    "# 1. 출력 구조 정의 (Pydantic Model)\n",
    "# LLM의 출력이 따르기를 원하는 JSON 스키마를 Pydantic 모델로 정의합니다.\n",
    "class ToolSelector(BaseModel):\n",
    "    \"\"\"사용자 질문을 가장 적절한 도구로 라우팅하는 역할의 데이터 모델.\"\"\"\n",
    "    \n",
    "    # tool 필드를 정의합니다. Literal을 사용하여 이 필드의 값은\n",
    "    # 'search_menu', 'search_web', 'search_wine' 셋 중 하나여야 함을 강제합니다.\n",
    "    # 이는 LLM이 잘못된 도구 이름을 생성하는 것을 방지합니다.\n",
    "    tool: Literal[\"search_menu\", \"search_web\", \"search_wine\"] = Field(\n",
    "        description=\"사용자의 질문을 기반으로 search_menu, search_wine, search_web 중 하나의 도구를 선택하세요.\",\n",
    "    )\n",
    "\n",
    "# 2. 구조화된 출력을 위한 LLM 설정 (with_structured_output 핵심 적용)\n",
    "# 기존 LLM에 with_structured_output을 적용하여 새로운 structured_llm을 만듭니다.\n",
    "# 이 함수는 LLM이 ToolSelector 클래스가 정의한 구조(JSON 스키마)를 따르는 객체를 출력하도록 강제합니다.\n",
    "structured_llm = llm.with_structured_output(ToolSelector)\n",
    "print(type(structured_llm)) # 출력 타입: LangChain Runnable\n",
    "\n",
    "# 3. 라우팅을 위한 프롬프트 템플릿\n",
    "# dedent를 사용하여 여러 줄의 시스템 프롬프트를 깔끔하게 정의합니다.\n",
    "system = dedent(\"\"\"당신은 사용자 질문을 적절한 도구로 라우팅하는 데 특화된 AI 어시스턴트입니다.\n",
    "다음 지침을 사용하십시오:\n",
    "- 레스토랑의 메뉴에 대한 질문에는 search_menu 도구를 사용하십시오.\n",
    "- 와인 추천이나 페어링 정보에는 search_wine 도구를 사용하십시오.\n",
    "- 기타 다른 정보나 최신 데이터에 대한 질문에는 search_web 도구를 사용하십시오.\n",
    "항상 사용자 질문을 기반으로 가장 적절한 도구를 선택하십시오.\"\"\")\n",
    "\n",
    "# 시스템 역할과 사용자 질문을 포함하는 ChatPromptTemplate을 생성합니다.\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"), # 사용자의 실제 질문이 여기에 삽입됩니다.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. 질문 라우터 정의 (체인 구성)\n",
    "# 프롬프트와 구조화된 LLM을 연결하여 라우팅 체인(question_router)을 완성합니다.\n",
    "# 이 체인을 실행하면, LLM은 프롬프트의 지침에 따라 ToolSelector 객체를 반환합니다.\n",
    "question_router = route_prompt | structured_llm\n",
    "print(type(question_router)) # 출력 타입: LangChain Runnable\n",
    "\n",
    "# 5. 테스트 실행\n",
    "# 'invoke'를 사용하여 각기 다른 질문에 대한 라우팅 결과를 확인합니다.\n",
    "print(question_router.invoke({\"question\": \"채식주의자를 위한 메뉴가 있나요?\"}))\n",
    "# 예상 출력: tool='search_menu' (메뉴에 대한 질문)\n",
    "\n",
    "print(question_router.invoke({\"question\": \"스테이크 메뉴와 어울리는 와인을 추천해주세요.\"}))\n",
    "# 예상 출력: tool='search_wine' (와인 추천에 대한 질문)\n",
    "\n",
    "print(question_router.invoke({\"question\": \"2022년 월드컵 우승 국가는 어디인가요?\"}))\n",
    "# 예상 출력: tool='search_web' (일반적인 최신 정보에 대한 질문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70feebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 라우팅 노드 \n",
    "# 상태 Schema 정의  class AdaptiveRagState(TypedDict):\n",
    "def route_question_adaptive(state: AdaptiveRagState) -> Literal[\"search_menu\", \"search_wine\", \"search_web\", \"llm_fallback\"]:\n",
    "    question = state[\"question\"]\n",
    "    try:\n",
    "        result = question_router.invoke({\"question\": question})\n",
    "        datasource = result.tool\n",
    "        \n",
    "        if datasource == \"search_menu\":\n",
    "            return \"search_menu\"\n",
    "        elif datasource == \"search_wine\":\n",
    "            return \"search_wine\"        \n",
    "        elif datasource == \"search_web\":\n",
    "            return \"search_web\"\n",
    "        else:\n",
    "            return \"llm_fallback\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in routing: {str(e)}\")\n",
    "        return \"llm_fallback\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8b91a",
   "metadata": {},
   "source": [
    "`(3) 검색 노드`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee53f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'AdaptiveRagState'는 LangGraph의 상태(State) 딕셔너리 타입을 나타내는 것으로 추정됩니다.\n",
    "# 이 딕셔너리는 일반적으로 \"question\", \"documents\" 등의 키를 포함합니다.\n",
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# AdaptiveRagState 타입을 정의\n",
    "# 여기서는 예시를 위해 최소한의 구조만 정의합니다.\n",
    "class AdaptiveRagState(TypedDict):\n",
    "    \"\"\"LangGraph의 상태를 정의하는 딕셔너리.\"\"\"\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    # 기타 상태 필드...\n",
    "\n",
    "# search_menu, search_wine, search_web은 실제 검색 엔진(예: Retriever)을 호출하는 Runnable 객체여야 합니다.\n",
    "# (이 객체들은 코드 외부에서 정의되어 있어야 합니다.)\n",
    "# search_menu = ... \n",
    "# search_wine = ...\n",
    "# search_web = ...\n",
    "\n",
    "\n",
    "def search_menu_adaptive(state: AdaptiveRagState):\n",
    "    \"\"\"\n",
    "    레스토랑 메뉴 내 정보를 검색하는 노드입니다.\n",
    "\n",
    "    Args:\n",
    "        state (AdaptiveRagState): 현재 LangGraph의 상태 딕셔너리.\n",
    "            여기에 'question' 키를 통해 사용자 질문이 포함되어 있습니다.\n",
    "\n",
    "    Returns:\n",
    "        dict: 업데이트된 상태 딕셔너리. 검색된 'documents' 리스트를 포함합니다.\n",
    "    \"\"\"\n",
    "    # 1. 상태에서 사용자 질문을 추출합니다.\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # 2. 메뉴 검색 도구(Retriever)를 호출하여 질문과 관련된 문서를 검색합니다.\n",
    "    # search_menu는 보통 LangChain의 Retriever 객체입니다.\n",
    "    docs = search_menu.invoke(question)\n",
    "\n",
    "    # 3. 검색 결과(문서 리스트)를 확인하고 상태를 반환합니다.\n",
    "    if len(docs) > 0:\n",
    "        # 관련 문서가 발견된 경우, 해당 문서를 'documents' 키로 반환합니다.\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        # 관련 문서가 없는 경우, 문서가 없음을 알리는 메시지를 담은 Document 객체를 반환합니다.\n",
    "        return {\"documents\": [Document(page_content=\"관련 메뉴 정보를 찾을 수 없습니다.\")]}\n",
    "\n",
    "\n",
    "def search_wine_adaptive(state: AdaptiveRagState):\n",
    "    \"\"\"\n",
    "    레스토랑의 와인 리스트 내 정보를 검색하는 노드입니다.\n",
    "\n",
    "    Args:\n",
    "        state (AdaptiveRagState): 현재 LangGraph의 상태 딕셔너리.\n",
    "\n",
    "    Returns:\n",
    "        dict: 업데이트된 상태 딕셔너리. 검색된 'documents' 리스트를 포함합니다.\n",
    "    \"\"\"\n",
    "    # 1. 상태에서 사용자 질문을 추출합니다.\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # 2. 와인 검색 도구(Retriever)를 호출하여 질문과 관련된 와인 문서를 검색합니다.\n",
    "    docs = search_wine.invoke(question)\n",
    "\n",
    "    # 3. 검색 결과(문서 리스트)를 확인하고 상태를 반환합니다.\n",
    "    if len(docs) > 0:\n",
    "        # 관련 문서가 발견된 경우, 해당 문서를 'documents' 키로 반환합니다.\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        # 관련 문서가 없는 경우, 문서가 없음을 알리는 메시지를 담은 Document 객체를 반환합니다.\n",
    "        return {\"documents\": [Document(page_content=\"관련 와인 정보를 찾을 수 없습니다.\")]}\n",
    "\n",
    "\n",
    "def search_web_adaptive(state: AdaptiveRagState):\n",
    "    \"\"\"\n",
    "    레스토랑 내부 정보 외의 일반 정보나 최신 정보를 웹에서 검색하고 결과를 반환하는 노드입니다.\n",
    "\n",
    "    Args:\n",
    "        state (AdaptiveRagState): 현재 LangGraph의 상태 딕셔너리.\n",
    "\n",
    "    Returns:\n",
    "        dict: 업데이트된 상태 딕셔너리. 검색된 'documents' 리스트를 포함합니다.\n",
    "    \"\"\"\n",
    "    # 1. 상태에서 사용자 질문을 추출합니다.\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # 2. 웹 검색 도구(Tool)를 호출하여 질문과 관련된 정보를 검색합니다.\n",
    "    docs = search_web.invoke(question)\n",
    "    \n",
    "    # 3. 검색 결과(문서 리스트)를 확인하고 상태를 반환합니다.\n",
    "    if len(docs) > 0:\n",
    "        # 관련 문서가 발견된 경우, 해당 문서를 'documents' 키로 반환합니다.\n",
    "        return {\"documents\": docs}\n",
    "    else:\n",
    "        # 관련 문서가 없는 경우, 문서가 없음을 알리는 메시지를 담은 Document 객체를 반환합니다.\n",
    "        return {\"documents\": [Document(page_content=\"관련 정보를 찾을 수 없습니다.\")]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19552a",
   "metadata": {},
   "source": [
    "`(4) 생성 노드`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f70286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# (AdaptiveRagState, llm 객체 등은 코드 외부에서 정의되었다고 가정합니다.)\n",
    "\n",
    "# RAG 프롬프트 정의\n",
    "# ChatPromptTemplate은 LLM에게 전달될 최종 메시지 형식을 정의합니다.\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    # 시스템 프롬프트: LLM에게 답변 생성 규칙과 역할을 부여합니다.\n",
    "    (\"system\", \"\"\"당신은 제공된 문서를 기반으로 질문에 답변하는 조수입니다. 다음 지침을 따르십시오:\n",
    "\n",
    "1. 제공된 문서의 정보만을 사용하십시오.\n",
    "2. 문서에 관련 정보가 부족하면 \"제공된 문서에는 이 질문에 답변할 정보가 포함되어 있지 않습니다.\"라고 말하십시오.\n",
    "3. 답변에 문서의 관련 부분을 인용하십시오.\n",
    "4. 추측하거나 문서에 없는 정보를 추가하지 마십시오.\n",
    "5. 답변을 간결하고 명확하게 유지하십시오.\n",
    "6. 관련 없는 정보는 생략하십시오.\"\"\"\n",
    "),\n",
    "    # 사용자(Human) 프롬프트: 검색된 문서와 실제 질문을 LLM에게 전달하는 템플릿입니다.\n",
    "    # {documents}와 {question}이 최종적으로 검색된 내용과 사용자 질문으로 대체됩니다.\n",
    "    (\"human\", \"다음 문서를 사용하여 다음 질문에 답하십시오:\\n\\n[문서]\\n{documents}\\n\\n[질문]\\n{question}\"),\n",
    "])\n",
    "\n",
    "# RAG 답변 생성 노드 함수\n",
    "def generate_adaptive(state: AdaptiveRagState):\n",
    "    \"\"\"\n",
    "    이전 노드에서 검색된 문서를 사용하여 최종 답변을 생성하는 노드입니다.\n",
    "\n",
    "    Args:\n",
    "        state (AdaptiveRagState): 현재 LangGraph의 상태 딕셔너리. \n",
    "            'question' (질문)과 'documents' (검색된 문서)를 포함합니다.\n",
    "\n",
    "    Returns:\n",
    "        dict: 업데이트된 상태 딕셔너리. 생성된 답변('generation')을 포함합니다.\n",
    "    \"\"\"\n",
    "    # 1. 상태(state)에서 필요한 정보(질문, 문서)를 안전하게 추출합니다.\n",
    "    question = state.get(\"question\", None)\n",
    "    documents = state.get(\"documents\", [])\n",
    "    \n",
    "    # 2. 'documents'가 단일 객체일 경우(LangGraph에서 종종 발생) 리스트로 변환하여 처리 일관성을 확보합니다.\n",
    "    if not isinstance(documents, list):\n",
    "        documents = [documents]\n",
    "\n",
    "    # 3. 검색된 문서 리스트를 LLM 프롬프트에 삽입할 수 있는 단일 문자열로 변환합니다.\n",
    "    # 각 문서의 본문(page_content)과 메타데이터(metadata)를 구분하여 문자열을 만듭니다.\n",
    "    documents_text = \"\\n\\n\".join([f\"---\\n본문: {doc.page_content}\\n메타데이터:{str(doc.metadata)}\\n---\" for doc in documents])\n",
    "\n",
    "    # 4. RAG 체인 구성 및 실행\n",
    "    # 프롬프트 템플릿(rag_prompt) -> LLM 호출(llm) -> 문자열 파서(StrOutputParser) 순으로 연결합니다.\n",
    "    rag_chain = rag_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # 구성된 체인에 필요한 변수(documents_text, question)를 전달하여 LLM을 호출합니다.\n",
    "    generation = rag_chain.invoke({\"documents\": documents_text, \"question\": question})\n",
    "    \n",
    "    # 5. 생성된 답변을 'generation' 키로 상태 딕셔너리에 추가하여 반환합니다.\n",
    "    return {\"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7312a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# (AdaptiveRagState, llm 객체 등은 코드 외부에서 정의되었다고 가정합니다.)\n",
    "\n",
    "# LLM Fallback 프롬프트 정의\n",
    "# 이 프롬프트는 RAG 문서 없이 LLM의 일반 지식에 의존하여 답변할 때 사용됩니다.\n",
    "fallback_prompt = ChatPromptTemplate.from_messages([\n",
    "    # 시스템 프롬프트: 일반 AI 어시스턴트로서의 역할을 정의하고 답변 지침을 부여합니다.\n",
    "    (\"system\", \"\"\"당신은 다양한 주제를 돕는 AI 어시스턴트입니다. 다음 지침을 따르십시오:\n",
    "\n",
    "1. 능력이 닿는 한 정확하고 유용한 정보를 제공하십시오.\n",
    "2. 확신이 없을 때는 불확실성을 표현하십시오. 추측은 피하십시오.\n",
    "3. 답변은 간결하지만 정보를 담고 있도록 유지하십시오.\n",
    "4. 사용자에게 필요하면 명확한 설명을 요청할 수 있다고 알리십시오.\n",
    "5. 윤리적이고 건설적으로 응답하십시오.\n",
    "6. 해당되는 경우 신뢰할 수 있는 일반 출처를 언급하십시오.\"\"\"),\n",
    "    # 사용자(Human) 프롬프트: 단순하게 사용자 질문만을 LLM에 전달합니다.\n",
    "    # 이 프롬프트는 {documents} 변수가 없다는 것이 RAG 프롬프트와의 주요 차이점입니다.\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "# LLM Fallback 답변 생성 노드 함수\n",
    "def llm_fallback_adaptive(state: AdaptiveRagState):\n",
    "    \"\"\"\n",
    "    검색된 문서(context) 없이 LLM의 일반적인 지식을 사용하여 답변을 생성하는 노드입니다.\n",
    "    이는 RAG가 실패했을 때의 최종 대안(fallback) 역할을 합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AdaptiveRagState): 현재 LangGraph의 상태 딕셔너리. \n",
    "            'question' 키를 통해 사용자 질문을 가져옵니다.\n",
    "\n",
    "    Returns:\n",
    "        dict: 업데이트된 상태 딕셔너리. LLM이 생성한 답변('generation')을 포함합니다.\n",
    "    \"\"\"\n",
    "    # 1. 상태(state)에서 사용자 질문을 추출합니다.\n",
    "    question = state.get(\"question\", \"\")\n",
    "    \n",
    "    # 2. LLM 체인 구성\n",
    "    # Fallback 프롬프트 -> LLM 호출(llm) -> 문자열 파서(StrOutputParser) 순으로 연결합니다.\n",
    "    llm_chain = fallback_prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # 3. 구성된 체인에 질문을 전달하여 LLM을 호출하고 답변을 생성합니다.\n",
    "    # 이 호출은 RAG 문서 없이 순수하게 LLM의 학습된 지식만을 사용합니다.\n",
    "    generation = llm_chain.invoke({\"question\": question})\n",
    "    \n",
    "    # 4. 생성된 답변을 'generation' 키로 상태 딕셔너리에 추가하여 반환합니다.\n",
    "    return {\"generation\": generation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388d9e3",
   "metadata": {},
   "source": [
    "`(5) 그래프 연결`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeabf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "#from IPython.display import Image, display\n",
    "\n",
    "# 그래프 구성 시작\n",
    "# StateGraph를 초기화합니다. 이 객체는 LangGraph 워크플로우의 설계도 역할\n",
    "# AdaptiveRagState는 이 그래프 전체에서 공유되고 업데이트될 데이터 구조(상태)의 스키마를 정의\n",
    "builder = StateGraph(AdaptiveRagState)\n",
    "\n",
    "# 1. 노드 추가 (Add Nodes)\n",
    "# LangGraph의 각 노드는 워크플로우 내에서 특정 작업을 수행하는 함수를 나타냅니다.\n",
    "builder.add_node(\"search_menu\", search_menu_adaptive) # 레스토랑 메뉴 검색 노드\n",
    "builder.add_node(\"search_wine\", search_wine_adaptive) # 와인 리스트 검색 노드\n",
    "builder.add_node(\"search_web\", search_web_adaptive)   # 일반 웹 검색 노드\n",
    "builder.add_node(\"generate\", generate_adaptive)       # 검색된 문서를 기반으로 답변을 생성하는 RAG 노드\n",
    "builder.add_node(\"llm_fallback\", llm_fallback_adaptive) # 검색 실패 시 LLM의 일반 지식으로 답변하는 폴백 노드\n",
    "\n",
    "# 2. 엣지 추가 (Add Edges)\n",
    "# 엣지는 노드 간의 흐름을 정의합니다.\n",
    "\n",
    "# 조건부 엣지 추가 (Conditional Edges)\n",
    "# 그래프의 시작점(START)에서 다음 노드로의 이동을 결정하는 조건부 라우터를 설정합니다.\n",
    "builder.add_conditional_edges(\n",
    "    START,\n",
    "    # 'route_question_adaptive' 함수를 라우터로 사용합니다. \n",
    "    # 이 함수는 사용자 질문을 분석하여 'search_menu', 'search_wine', 'search_web', 'llm_fallback' 중 하나의 노드 이름을 반환\n",
    "    route_question_adaptive,\n",
    "    # mapping은 route_question_adaptive의 출력(노드 이름)과 실제 노드를 연결합니다.\n",
    "    # 이 예시에서는 route_question_adaptive가 노드 이름을 직접 반환한다고 가정하므로, \n",
    "    # 추가적인 매핑은 생략되었거나, route_question_adaptive 함수 자체 내에 로직이 포함되어 있습니다.\n",
    ")\n",
    "\n",
    "# 일반 엣지 추가 (Fixed Edges)\n",
    "# 검색 노드가 완료되면, 그 결과(documents)는 답변 생성 노드('generate')로 무조건 이동합니다.\n",
    "builder.add_edge(\"search_menu\", \"generate\")\n",
    "builder.add_edge(\"search_wine\", \"generate\")\n",
    "builder.add_edge(\"search_web\", \"generate\")\n",
    "\n",
    "# 답변 생성 노드가 완료되면, 그래프 실행을 종료합니다.\n",
    "builder.add_edge(\"generate\", END)\n",
    "\n",
    "# LLM Fallback 노드가 완료되어도, 그것이 최종 답변이므로 그래프 실행을 종료합니다.\n",
    "builder.add_edge(\"llm_fallback\", END)\n",
    "\n",
    "# 3. 그래프 컴파일 (Compile Graph)\n",
    "# 정의된 노드와 엣지를 기반으로 실행 가능한 LangChain Runnable 객체로 그래프를 빌드합니다.\n",
    "# 이 컴파일된 객체('adaptive_rag')는 .invoke() 메서드를 사용하여 실제 워크플로우를 실행합니다.\n",
    "adaptive_rag = builder.compile()\n",
    "\n",
    "# 그래프 시각화\n",
    "#display(Image(adaptive_rag.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mermaid_code = adaptive_rag.get_graph().draw_mermaid()\n",
    "print(\"Mermaid Code:\")\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8552cb",
   "metadata": {},
   "source": [
    "* https://mermaid.live/ 에서  mermain_code 로 직접 확인한다.\n",
    "\n",
    "* [Graph이미지](https://mermaidchart.com/play?utm_source=mermaid_live_editor&utm_medium=share#pako:eNp9kt1ygjAQhV-F2d7gjFgMKhg63tRH6FVLx4mwEaYhMCFMfxzfvStVirXtFbv5djfnLNlDWmUIHHZG1LnzsI4TndjNprHC0Md9uqtXfXZ3W6-eR5xzWZjGHgsbFCbNNyXq1h3EowF7LTS6g_iC4db9DjuyQ41GWHTPQXeqVLmRQqmtSF_cYTL6kos668V2cS9ViS-lvQnHm3grZzgj_oUPzPyDSfR_lNzGP7bkeMTP1uLLVfzNaNA1PCcdOdmOfy7rCqa0kGaN0slQilZZRxZK8RvJpC_lWNFNXo7FLrd8OmEXDd1P78q9qhZpYd-5f1FwXPVp3FZuFzKFMT2rIgNOahocQ4mmFMcc9ol2nARsjiUmwCk8yUkg0Qfqq4V-rKoSuDUtdZqq3eXnpK0zsr0uBL3Zsh9uyCOa-6rVFjibdyOA7-ENeEhWwsBn82C5mAazJcF34NNZNIkWLFqyeRTMQhYdxvDR3elPwjBgIWPTRcB8PwijwyfsSxe-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39203d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행\n",
    "inputs = {\"question\": \"스테이크 메뉴의 가격은 얼마인가요?\"}\n",
    "for output in adaptive_rag.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Node '{key}':\")\n",
    "        print(f\"State '{value.keys()}':\")\n",
    "        print(f\"Value '{value}':\")\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "# 최종 답변\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c15796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행\n",
    "inputs = {\"question\": \"푸이 퓌세 2019의 주요 품종은 무엇인가요?\"}\n",
    "for output in adaptive_rag.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Node '{key}':\")\n",
    "        print(f\"State '{value.keys()}':\")\n",
    "        pprint(f\"Value '{value}':\")\n",
    "        #pprint(f\"Value '{value.page_content}':\")\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "# 최종 답변\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67a568",
   "metadata": {},
   "source": [
    "### 3-2. 사람의 개입 (Human-in-the-Loop)\n",
    "\n",
    "* Human-in-the-Loop (HITL)는 AI 시스템에 인간의 판단과 개입을 통합하는 접근 방식\n",
    "* AI의 자동화된 처리와 인간의 전문성을 결합하여 더 정확하고 신뢰할 수 있는 결과를 도출하는 것을 목표\n",
    "#### MemorySaver의 역할 \n",
    "* MemorySaver 클래스는 LangGraph에서 **체크포인팅(Checkpointing)**을 구현하는 가장 기본적인 방법 중 하나로 사용됩니다. \n",
    "* 체크포인팅은 복잡한 워크플로우를 실행하는 데 있어 필수적인 기능입니다.\n",
    "* 1. 상태 저장 및 복원 (Checkpointing)\n",
    "    * MemorySaver의 주된 역할은 LangGraph의 **실행 상태(State)**를 저장하는 것입니다.\n",
    "        * 저장 (Saving): 그래프의 실행이 특정 노드를 지날 때마다, 그 시점의 상태(AdaptiveRagState와 같은 딕셔너리)를 **메모리(RAM)**에 저장합니다.\n",
    "        * 복원 (Restoring): 나중에 동일한 **스레드 ID (thread ID)**를 사용하여 그래프를 호출하면, MemorySaver는 이전에 저장된 마지막 상태를 불러와서 그 지점부터 실행을 재개할 수 있도록 합니다.\n",
    "* 2. 메모리 기반 저장 (In-Memory)\n",
    "    * MemorySaver는 데이터를 RAM에 저장합니다. 이는 매우 빠르다는 장점이 있지만, Python 세션이 종료되면 저장된 모든 체크포인트 데이터도 사라진다는 단점이 있습니다.         \n",
    "    * 따라서 이는 주로 개발, 테스트, 데모 목적으로 사용되며, 프로덕션 환경이나 영구적인 상태 저장이 필요할 때는 SQLAlchemySaver와 같은 **영구적인 저장소(데이터베이스)**를 사용하는 체크포인트 구현체를 사용해야 합니다.\n",
    "* 3. 대화 기록 유지 (Conversational Memory)\n",
    "    * LangGraph가 실행의 연속성을 가지고 이전 상태를 기억하여 대화를 재개하거나 실패한 지점부터 작업을 다시 시작할 수 있도록 돕는 휘발성(Volatile) 메모리 기반의 상태 관리 도구입니다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f516a1f",
   "metadata": {},
   "source": [
    "`(1) 체크포인트 설정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced52a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06be5d",
   "metadata": {},
   "source": [
    "`(2) Breakpoint 추가`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5db53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컴파일 - 'generate' 노드 전에 중단점 추가\n",
    "# ( builder는 이전에 정의된 StateGraph 객체이고, memory는 MemorySaver 객체이며, 'generate'는 답변 생성 노드의 이름입니다.)\n",
    "\n",
    "# 컴파일 - 'generate' 노드 전에 중단점 추가\n",
    "# LangGraph의 빌더(builder)를 최종 실행 가능한 그래프로 컴파일합니다.\n",
    "adaptive_rag_hitl = builder.compile(\n",
    "    checkpointer=memory,             # 1. 체크포인터 설정: \n",
    "                                     #    그래프의 모든 중간 상태를 'memory' 객체(MemorySaver)에 저장하도록 설정합니다.\n",
    "                                     #    이를 통해 나중에 실행을 재개하거나 상태를 검토할 수 있습니다.\n",
    "    interrupt_before=[\"generate\"]    # 2. 인터럽트 설정 (Human-in-the-Loop 핵심):\n",
    "                                     #    워크플로우가 'generate'라는 이름의 노드를 실행하기 직전에 멈추도록 지시합니다.\n",
    "                                     #    이 중단점에서 사용자는 검색된 문서(documents)를 검토하거나 수정할 수 있습니다.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d986232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 그래프 출력\n",
    "#display(Image(adaptive_rag_hitl.get_graph().draw_mermaid_png()))\n",
    "mermaid_code = adaptive_rag_hitl.get_graph().draw_mermaid()\n",
    "print(\"Mermaid Code:\")\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d502f9",
   "metadata": {},
   "source": [
    "* https://mermaid.live/ 에서  mermain_code 로 직접 확인한다.\n",
    "\n",
    "* [Graph이미지](https://mermaidchart.com/play?utm_source=mermaid_live_editor&utm_medium=share#pako:eNp9Ul2PmzAQ_CuW-0IkSIhJAjGUl-Yn3NMdVWRgHVCNQcaovUb577chH0fu2nvyjGZ3vTPaIy3aEiinByO6ijzt4kxndr_vrTD4OC9Jl95ZsujSnzPOuaxNb8-FPQhTVPsG9OBM8Gyi_a41OBP8oEHuvMNROYAGIyw4N5BUZpEmfSOUShNocJtaWzBm6Cz5TnKQrYFkgUKyuBSNY5Rq9hJZLopfzpTMLv5Al3d3I757U-Ji7e6aeHMvJdMZ8T_0ifsvZHT5lYrxxB9iJR7qtyzix-z-r-Ggz-KNjMrVdvwxrE9igYH0O5CkBCkGZYmsleLfJJO-lK7Cn7wK6kNl-XLOHhrGKxnLvbYTRW1fuf9QcI76Oi6X-UYW1MU7rEvKcZseXNqAacSZ02OmCcmoraCBjHKE13UymukT9nVCP7dtQ7k1A3aadjhUNzJ0Jdre1QKP_L0CLYL50Q7aUs624wTKj_QP5SE6CQOfrYPtZhmstmuXvlK-XEXzaMOiLVtHwSpk0cmlf8cv_XkYBixkbLkJmO8HYXR6A5WEKLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089fbc90",
   "metadata": {},
   "source": [
    "`(3) Breakpoint 실행 확인`\n",
    "* LangGraph의 Human-in-the-Loop (HITL) 기능을 활용하여 워크플로우를 실행하고, 이전에 설정한 중단점에서 실행을 일시 정지시키는 과정을 보여줍니다. \n",
    "* MemorySaver와 thread_id를 사용하여 상태를 저장하고, stream을 통해 실시간으로 실행 흐름을 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a445aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Note: adaptive_rag_hitl은 'interrupt_before=[\"generate\"]'로 컴파일된 LangGraph 객체입니다.)\n",
    "from pprint import pprint\n",
    "\n",
    "# 1. 스레드(Thread) 설정\n",
    "# LangGraph 실행을 위한 설정(Config)을 정의합니다.\n",
    "thread = {\n",
    "    \"configurable\": {\n",
    "        # 'thread_id'는 이 특정 실행의 고유 ID입니다. \n",
    "        # 이전에 설정한 MemorySaver(checkpointer)는 이 ID를 사용하여 \n",
    "        # 그래프의 모든 중간 상태(체크포인트)를 저장하고 관리합니다. \n",
    "        # 이를 통해 나중에 멈춘 지점부터 정확하게 실행을 재개할 수 있습니다.\n",
    "        \"thread_id\": \"breakpoint_test\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. 입력 설정\n",
    "# 그래프 실행에 사용할 사용자 질문을 딕셔너리 형태로 정의합니다.\n",
    "inputs = {\"question\": \"스테이크 메뉴의 가격은 얼마인가요?\"}\n",
    "\n",
    "# 3. 그래프 실행 및 스트리밍 (Execution and Streaming)\n",
    "# adaptive_rag_hitl.stream()을 호출하여 그래프를 실행합니다.\n",
    "# 'stream' 방식은 노드에서 노드로 이동할 때마다 이벤트를 반환하여 실시간 진행 상황을 보여줍니다.\n",
    "# config=thread를 전달하여, 실행이 'breakpoint_test' 스레드에 속하도록 합니다.\n",
    "for event in adaptive_rag_hitl.stream(inputs, config=thread):\n",
    "    \n",
    "    # 4. 이벤트 출력 루프\n",
    "    # 각 이벤트는 {'노드_이름': {상태_업데이트}} 형태의 딕셔너리입니다.\n",
    "    for k, v in event.items():\n",
    "        \n",
    "        # LangGraph는 실행이 완전히 끝났을 때 '__end__'라는 이벤트를 반환합니다.\n",
    "        # 이 루프의 목적은 중단점 이전까지의 중간 과정을 확인하는 것이므로, 최종 이벤트는 건너뜁니다.\n",
    "        if k != \"__end__\":\n",
    "            # 현재 처리 중인 노드 이름(k)과 해당 노드가 반환한 상태 업데이트 값(v)을 출력합니다.\n",
    "            # (예: 라우팅 결정, 검색 노드가 반환한 documents 리스트)\n",
    "            pprint(f\"{k}: {v}\")  # 이벤트의 키와 값을 함께 출력\n",
    "            \n",
    "# 5. 실행의 최종 상태\n",
    "# 이 코드는 실행 중 'generate' 노드 직전에 설정된 중단점에서 멈춥니다.\n",
    "# 결과적으로 'search_menu' 노드까지의 실행 결과를 출력하고, \n",
    "# 'breakpoint_test' ID로 현재 상태가 체크포인트에 저장된 채로 종료됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a523e8",
   "metadata": {},
   "source": [
    "`(4) Breakpoint 상태 관리`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Note: adaptive_rag_hitl은 컴파일된 LangGraph 객체이며, \n",
    "# thread는 이전에 실행을 시작했던 {\"configurable\": {\"thread_id\": \"breakpoint_test\"}} 딕셔너리입니다.)\n",
    "\n",
    "# 1. 현재 그래프 상태 가져오기 (Get Current State)\n",
    "# 'get_state' 메서드를 사용하여 특정 스레드 ID('breakpoint_test')에 저장된 \n",
    "# 가장 최근의 체크포인트 상태(이 경우에는 중단점에서 멈춘 상태)를 불러옵니다.\n",
    "current_state = adaptive_rag_hitl.get_state(thread)\n",
    "\n",
    "# 2. 상태 출력 및 확인\n",
    "print(\"---그래프 상태---\")\n",
    "# 불러온 current_state 객체는 LangGraph의 State Snapshot을 나타냅니다.\n",
    "# 여기에는 질문, 검색된 문서, 실행 경로 등 모든 정보가 포함되어 있습니다.\n",
    "print(current_state)\n",
    "for doc in current_state.values['documents']:\n",
    "    print(doc)\n",
    "print(\"-\"*50)\n",
    "\n",
    "# 3. 특정 상태 값 확인 (Generation)\n",
    "# current_state.values는 실제 상태 변수(딕셔너리)를 담고 있습니다.\n",
    "# 'generation' 키의 값을 가져오려고 시도합니다.\n",
    "# **중요:** 이 시점('generate' 노드 이전)에서는 'generation'이 아직 생성되지 않았으므로,\n",
    "# 일반적으로 이 값은 None이거나 해당 키가 없을 수 있습니다. \n",
    "# 이 코드는 상태에 'generation'이 있는지 확인하려는 의도로 사용될 수 있습니다.\n",
    "print(current_state.values.get(\"generation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07987daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음에 실행될 노드를 확인 \n",
    "current_state.next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac16239",
   "metadata": {},
   "source": [
    "`(5) Breakpoint 이후 단계를 계속해서 실행`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c14bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Note: adaptive_rag_hitl은 이전에 'interrupt_before=[\"generate\"]'로 컴파일된 LangGraph 객체입니다.)\n",
    "# (thread는 이전에 실행을 멈췄던 스레드의 ID를 가진 {\"configurable\": {\"thread_id\": \"breakpoint_test\"}} 딕셔너리입니다.)\n",
    "\n",
    "# 1. 중단된 실행 재개 및 스트리밍\n",
    "# adaptive_rag_hitl.stream()을 다시 호출하여 실행을 재개합니다.\n",
    "# 첫 번째 인수를 None으로 설정하는 것이 중요합니다:\n",
    "# - None: 새로운 입력을 제공하지 않고, 이전에 'thread'에 저장된 상태를 기반으로 실행을 재개하라는 의미입니다.\n",
    "# - config=thread: 이전에 중단되었던 'breakpoint_test' 스레드의 상태를 불러오도록 지정합니다.\n",
    "for event in adaptive_rag_hitl.stream(None, config=thread):\n",
    "    \n",
    "    # 2. 이벤트 출력 루프\n",
    "    # 재개된 실행의 이벤트(노드 실행 결과)를 실시간으로 확인합니다.\n",
    "    for k, v in event.items():\n",
    "        \n",
    "        # LangGraph의 최종 종료 이벤트는 건너뜁니다.\n",
    "        if k != \"__end__\":\n",
    "            # 현재 실행 중인 노드 이름(k)과 해당 노드가 반환한 상태 업데이트 값(v)을 출력합니다.\n",
    "            # 이 출력은 중단점 이후의 실행, 즉 'generate' 노드의 실행 결과를 보여줍니다.\n",
    "            print(f\"{k}: {v}\")  # 이벤트의 키와 값을 함께 출력\n",
    "            \n",
    "# 3. 실행의 최종 상태\n",
    "# 이 코드를 실행하면, 그래프는 이전에 멈췄던 지점('generate' 노드 직전)에서 시작하여,\n",
    "# 'generate' 노드를 실행하고 최종 답변을 생성한 후, 'END' 노드로 이동하며 실행을 완료합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Note: adaptive_rag_hitl은 컴파일된 LangGraph 객체이며, \n",
    "# thread는 이전에 실행을 멈췄던 스레드의 ID를 가진 딕셔너리입니다.)\n",
    "\n",
    "# 1. 현재 그래프 상태 가져오기 (Get Current State)\n",
    "# 'get_state' 메서드를 사용하여 특정 스레드 ID에 저장된 가장 최근의 체크포인트 상태를 불러옵니다.\n",
    "# 이 상태는 이전에 'interrupt_before=[\"generate\"]' 설정에 의해 멈춘 지점의 정보입니다.\n",
    "current_state = adaptive_rag_hitl.get_state(thread)\n",
    "\n",
    "# 2. 다음에 실행될 노드 확인\n",
    "# 불러온 current_state 객체에는 현재 상태 외에도 다음에 실행될 노드의 이름이 저장되어 있습니다.\n",
    "# 'next' 속성은 실행이 재개될 경우 가장 먼저 호출될 노드의 이름(들)을 리스트 형태로 반환합니다.\n",
    "# 이 경우, 이전에 중단된 지점이 'generate' 노드 직전이었으므로, 출력은 ['generate']가 됩니다.\n",
    "current_state.next\n",
    "# print(current_state.next) # 예시 출력: ['generate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e533e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Note: adaptive_rag_hitl은 컴파일된 LangGraph 객체이며, \n",
    "# thread는 이전에 실행을 시작했던 스레드 ID를 가진 딕셔너리입니다.)\n",
    "\n",
    "# 최종 답변 확인\n",
    "# 1. 현재 그래프 상태 가져오기\n",
    "# 'get_state' 메서드를 사용하여 이전에 중단되었거나 저장된 특정 스레드 ID의 \n",
    "# 가장 최근 체크포인트 상태(State Snapshot)를 불러옵니다.\n",
    "current_state = adaptive_rag_hitl.get_state(thread)\n",
    "\n",
    "# 2. 'generation' 필드의 값 출력\n",
    "# current_state.values는 상태 딕셔너리(question, documents 등)를 담고 있습니다.\n",
    "# .get(\"generation\")을 사용하여 최종 답변 필드의 값을 가져와 출력합니다.\n",
    "# 주의: 이전에 'interrupt_before=[\"generate\"]' 설정으로 멈췄다면, \n",
    "# 'generate' 노드가 실행되지 않았으므로, 이 값은 보통 None이 됩니다.\n",
    "print(current_state.values.get(\"generation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8964592",
   "metadata": {},
   "source": [
    "`(6) 상태 업데이트`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c1e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 thread를 생성하고, 새로운 질문을 수행 \n",
    "thread = {\"configurable\": {\"thread_id\": \"breakpoint_update\"}}\n",
    "inputs = {\"question\": \"매운 음식이 있나요?\"}\n",
    "for event in adaptive_rag_hitl.stream(inputs, config=thread):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(f\"{k}: {v}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee35fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음에 실행될 노드를 확인 \n",
    "current_state = adaptive_rag_hitl.get_state(thread)\n",
    "current_state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af930c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question, generation 필드 확인\n",
    "current_state = adaptive_rag_hitl.get_state(thread)\n",
    "print(current_state.values.get(\"question\"))\n",
    "print(\"-\"*50)\n",
    "print(current_state.values.get(\"generation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 업데이트 - 질문을 수정하여 업데이트\n",
    "adaptive_rag_hitl.update_state(thread, {\"question\": \"매콤한 해산물 요리가 있나요?\"})\n",
    "\n",
    "# 상태 확인\n",
    "new_state = adaptive_rag_hitl.get_state(thread)\n",
    "\n",
    "print(new_state.values.get(\"question\"))\n",
    "print(\"-\"*50)\n",
    "print(new_state.values.get(\"generation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb057b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값을 None으로 지정하면 중단점부터 실행하고 최종 답변을 생성 \n",
    "for event in adaptive_rag_hitl.stream(None, config=thread):\n",
    "    for k, v in event.items():\n",
    "        # '__end__' 이벤트는 미출력\n",
    "        if k != \"__end__\":\n",
    "            print(f\"{k}: {v}\")  # 이벤트의 키와 값을 함께 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 답변 확인\n",
    "print(event[\"generate\"][\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-0lCeHk3W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
